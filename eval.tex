\section{Experimental Evaluation}

We evaluate the performance of the semi-external memory sparse matrix
multiplication on multiple real-world billion-scale graphs including a web-page
graph with 3.4 billion vertices. We first demonstrate the effectiveness of
both in-memory and external-memory optimizations on sparse matrix multiplication
and then compare the performance of our semi-external-memory implementation with
multiple in-memory implementations: \textit{(i)} our in-memory implementation,
\textit{(ii)} MKL and \textit{(iii)} Trilinos. We then evaluate the overall
performance of the applications that require sparse matrix multiplication.
We further demonstrate the scalability of our implementations on a web graph
of 3.4 billion vertices and 129 billion edges.

We conduct all experiments on a non-uniform memory architecture machine with
four Intel Xeon E7-4860 processors, clocked at 2.6 GHz, and 1TB memory of
DDR3-1600. Each processor has 12 cores. The machine has three LSI SAS 9300-8e
host bus adapters (HBA) connected to a SuperMicro storage chassis, in which
24 OCZ Intrepid 3000 SSDs are installed. The 24 SSDs together are capable of
delivering 12 GB/s for read and 10 GB/s for write at maximum. The machine runs
Linux kernel v3.13.0. We use 48 threads for most of experiments by default.

\begin{table}
\begin{center}
\footnotesize
\begin{tabular}{|c|c|c|c|c|}
\hline
Graph datasets & \# Vertices & \# Edges & Directed \\
\hline
Twitter \cite{twitter} & $42$M & $1.5$B & Yes \\
\hline
Friendster \cite{friendster} & $65$M & $1.7$B & No \\
\hline
Page graph \cite{web_graph} & $3.4$B & $129$B & Yes \\
\hline
RMAT-40 \cite{rmat} & 100M & 3.7B & No \\
\hline
RMAT-160 \cite{rmat} & 100M & 14B & No \\
\hline
\end{tabular}
\normalsize
\end{center}
\caption{Graph data sets.}
\label{graphs}
\end{table}

We use some synthetic graphs and real-world graphs, as shown in Table \ref{graphs}
for evaluation. The smallest graph we use has 42 million vertices and 1.5 billion
edges. The largest graph is the page graph with 3.4 billion vertices
and 129 billion edges, which is two orders of magnitude larger than the smallest
graphs. We generate two synthetic graphs with R-Mat \cite{rmat} to fill
the size gap between the smallest graph and the largest graph. The synthetic graphs
have different density to help evaluate our sparse matrix multiplication more
thoroughly. The page graph is clustered by domain, generating good CPU cache
hit rates in sparse matrix multiplication.

\subsection{Optimizations on sparse matrix multiplication}
We first deploy a set of memory optimizations to implement a fast
in-memory sparse matrix multiplication and then deploy a set of I/O
optimizations to further speed up this operation in semi-external memory.
We apply the memory and I/O optimizations incrementally to illustrate their
effectiveness.

For memory optimizations, we start with an implementation that performs sparse
matrix multiplication on a sparse matrix in the CSR format and apply
the optimizations incrementally in the following order:
\begin{itemize} \itemsep1pt \parskip0pt \parsep0pt
		\item partition dense matrices for NUMA (\textit{NUMA}),
	\item partition the sparse matrix in both dimensions into tiles of
		$16K \times 16K$ (\textit{Cache blocking}),
	\item organize multiple physical tiles into a super tile to fill CPU cache
		(\textit{Super tile}),
	\item use CPU vectorization instructions (\textit{Vec}),
	\item allocate a local buffer to store the intermediate result of
		multiplication of tiles and the input dense matrix(\textit{Local write}),
	\item combine the SCSR and COO format to reduce the number of conditional
		jump CPU instructions (\textit{SCSR+COO}),
\end{itemize}

Figure \ref{perf:spmm_opt} shows that almost all of these optimizations have
positive effect on sparse matrix multiplication and all optimizations
together speed up the operation by $2-4$ times.
The degree of effectiveness varies
significantly between different graphs and different numbers of columns in
the dense matrices. For example, the NUMA optimization is more effective when
the dense matrices have more columns because more columns in the dense matrices
require more memory bandwidth. Cache blocking is very effective when
the dense matrices have fewer columns because it can effectively increase CPU
cache hits. When there are more columns in the dense matrices, data locality
improves and the effectiveness of cache blocking becomes less noticeable.
When there are too many columns, the rows from
the input and output matrices can no longer be in the CPU cache. Thus, it even
has a little negative effect on the Friendster graph when the dense matrices
have $16$ columns. With all optimizations, we have a fast in-memory sparse
matrix dense matrix multiplication, denoted by IM-SpMM.

\dz{We might want to show the effectiveness of load balancing as well.}

\begin{figure}
	\begin{center}
		\footnotesize
		\include{SpMM_optimize}
		\caption{The effectiveness of the SpMM optimizations on the Friendster
			graph (F) and the Twitter graph (T) for different numbers of
			columns in the dense matrices.}
		\label{perf:spmm_opt}
	\end{center}
\end{figure}

We modify the IM-SpMM implementation above for the semi-external memory sparse
matrix multiplication (SEM-SpMM) on SSDs and write the output dense matrix to
SSDs. We apply the I/O optimizations to sparse matrix vector multiplication
in the following order to show their effectiveness:
\begin{itemize} \itemsep1pt \parskip0pt \parsep0pt
	\item compress a sparse matrix,
	\item use one I/O thread per NUMA node (\textit{1IOT}),
	\item use I/O polling in worker threads (\textit{polling}),
	\item use a per-thread buffer pool to allocate memory for I/O
		(\textit{buf pool}),
	\item use 8MB for the maximal block size in the kernel (\textit{max block}),
	\item order the portions of the output dense matrix globally and stream them
		to SSDs with large I/O (\textit{global output}).
\end{itemize}

Sparse matrix vector multiplication requires substantial I/O throughput and
any improvement on I/O improves the overall performance of this operation.
Compressing the sparse matrix helps us to accelerate the information retrieval
of the sparse matrix and gives us the best performance boost among all
optimizations. By reducing the number of thread context switches, we can
also achieve substantial performance improvement. We can also see that
memory allocation with \textit{mmap()} also causes significant overhead
in high-throughput I/O access, so we should use a buffer pool to reduce
memory allocation when accessing SSDs.

\subsection{The performance of sparse matrix multiplication}

We evaluate the performance of SEM-SpMM and compare its performance with
our in-memory implementation (IM-SpMM), the MKL implementation and the Trilinos
implementation (Figure \ref{perf:spmm}). The MKL and Trilinos SpMM cannot run
on the page graph.

We first compare the performance of SEM-SpMM against IM-SpMM on all graphs with
the input dense matrix preloaded in memory. The experiments show that
SEM-SpMM gets XX-100\% performance of IM-SpMM. The performance gap becomes
smaller when the number of columns in the dense matrices increase in all graphs
and for both cases (the output dense matrix in memory and on SSDs) (Figure
\ref{perf:spmm_comp}).
We also show the average I/O throughput generated by SEM-SpMM in
Figure \ref{}. Sparse matrix vector multiplication generates very high I/O
throughput. It may get close to the bandwidth of the SSD array, which suggests
that I/O be the bottleneck of SEM-SpMM when the number of columns in the dense
matrices is small. As the number of columns increase, the I/O throughput
generated by SEM-SpMM is only a small fraction of the bandwidth of SSDs, which
suggests that CPU becomes the bottleneck of the system.

We further measure the performance of SEM-SpMM when the input dense matrix
cannot fit in memory (Figure \ref{}). We fix the number of columns in the dense
matrix, but vary the number of columns that can fit in memory to show
the impact of the memory size on the performance of SpMM. In this case, we need
to load the input dense matrix from SSDs and write the output dense matrix to
SSDs. When the memory
is small, doubling the memory size gives significant performance improvement.
As the memory size reaches XX, further doubling the memory size does not
give much performance improvement.

Both IM-SpMM and SEM-SpMM significantly outperform the MKL and Trilinos implementations.
Figure \ref{perf:spmm} shows the performance of different implementations of
sparse matrix multiplication with different numbers of columns in the dense
matrices on the Friendster graph. The performance result is very similar
on other graphs. The Trilinos SpMM is optimized for sparse matrix vector
multiplication (SpMV). But even for SpMV, both IM-SpMM and SEM-SpMM outperform
Trilinos. The MKL SpMM performs better when the dense matrices have more columns,
but our implementations can still outperform MKL by $2-3$ times in most settings.
\dz{I should add another line for distributed Trilinos.}

\begin{figure}
	\begin{center}
		\footnotesize
		\include{spmm.im.vs.sem}
		\caption{The performance of SEM-SpMM with different numbers of columns
			in the dense matrix on graphs in Table \ref{graphs} relative to IM-SpMM.}
		\label{perf:spmm_comp}
	\end{center}
\end{figure}

\begin{figure}
	\begin{center}
		\footnotesize
		\include{SpMM-Friendster}
		\caption{The runtime of IM-SpMM, SEM-SpMM, the MKL and the Trilinos
			implementations on the Friendster graph.}
		\label{perf:spmm}
	\end{center}
\end{figure}

%\begin{figure}[t]
%\centering
%\footnotesize
%\begin{subfigure}{.5\linewidth}
%	\include{spmm1}
%	\caption{SpMV}
%	\label{fig:spmm1}
%\end{subfigure}%
%\begin{subfigure}{.5\linewidth}
%	\include{spmm4}
%	\caption{SpMM}
%	\label{fig:spmm4}
%\end{subfigure}
%\caption{The performance of Trilinos and FlashEigen-SEM sparse matrix
%multiplication relative to FlashEigen-IM sparse matrix multiplication.
%In sparse matrix dense multiplication (SpMM), there are four columns
%in the dense matrix.}
%\label{fig:spmm}
%\end{figure}

%\begin{figure}
%	\footnotesize
%	\begin{subfigure}{.2\textwidth}
%		\input{SpMM-Friendster.tex}
%		\caption{A subfigure}
%		\label{fig:sub1}
%	\end{subfigure}%
%	\begin{subfigure}{.2\textwidth}
%		\input{SpMM-W0.tex}
%		\caption{A subfigure}
%		\label{fig:sub2}
%	\end{subfigure}
%	\caption{A figure with two subfigures}
%	\label{fig:test}
%\end{figure}

\subsection{Performance of the applications}

We evaluate the performance of the implementations of the applications in
Section \ref{sec:apps}. We compare the performance of our implementations
with the state-of-art implementations on smaller graphs. For each application,
we further show their scalability on the page graph.

\subsubsection{PageRank}
We first evaluate the performance of our SEM PageRank implementation when
it uses different amount of memory for computation. We compare the performance
of our implementation with the PageRank implementations in FlashGraph
\cite{flashgraph} and GraphLab create \cite{}.

\subsubsection{Eigensolver}

We evaluate the performance of our SEM eigensolver and compare its performance
with our in-memory eigensolver and the original KrylovSchur eigensolver.
We compute different numbers of eigenvalues on the three smaller graphs in
Table \ref{graphs}. Only our SEM eigensolver is able to compute eigenvalues
on the page graph on the 1TB-memory machine.

\begin{figure}[t]
\centering
\footnotesize
\begin{subfigure}{.5\linewidth}
	\include{eigen-runtime-8ev}
	\caption{8 eigenvalues.}
	\label{fig:eigen8}
\end{subfigure}%
\begin{subfigure}{.5\linewidth}
	\include{eigen-runtime-32ev}
	\caption{32 eigenvalues.}
	\label{fig:eigen32}
\end{subfigure}
\caption{The performance of the Trilinos KrylovSchur and our SEM KrylovSchur
relative to the our in-memory KrylovSchur.}
\label{fig:eigen}
\end{figure}

Our SEM eigensolver achieves at least 30\% performance of our in-memory
eigensolver, while the in-memory eigensolver outperforms the original
KrylovSchur eigensolver (Figure \ref{perf:eigen_rt}). The SEM eigensolver
is more efficient to compute a small number of eigenvalues and is able
to achieve around 50\% performance of our in-memory eigensolver.
The performance gap between the in-memory and SEM eigensolvers with the number
of eigenvalues to compute, and the increase of the performance gap can be
explained by Figure \ref{perf:eigen_decompose}. SpMM and reorthogonalization
account for most of computation time when computing a small number of eigenvalues,
but reorthogonalization eventually dominates the eigendecomposition for computing
many eigenvalues. Because external-memory dense matrix multiplication is several
times slower than the in-memory implementations, reorthogonalization accounts for
over 90\% of runtime in the SEM eigensolver for computing a large number of
eigenvalues. For many spectral analysis tasks, users only require a small number
of eigenvalues.

%\begin{figure}
%	\begin{center}
%		\footnotesize
%		\include{eigen-mem}
%		\caption{The memory consumptioin of the three KrylovSchur
%		implementations.}
%		\label{perf:eigen_mem}
%	\end{center}
%\end{figure}

The SEM eigensolver uses a small fraction of memory used by its in-memory
counterparts and the original Trilinos eigensolver and its memory consumption
remains roughly the same as the number of eigenvalues computed by the eigensolvers
increases. A small memory consumption provides two benefits. First, FlashEigen
is able to scale to a much larger eigenvalue problem. The second benefit is that
FlashEigen gives users more freedom to choose the subspace size that gives the
fastest convergence in a given eigenvalue problem because SSDs significantly
increases memory available to the eigensolver. We have to point out that our
experiments assume that we choose the subspace size that is not constrained by
the memory size in a machine.

FlashEigen has memory consumption linear to the number of vertices in a graph.
It keeps two $n \times b$ dense matrices in memory and each worker thread
keeps a fixed number of buffers for I/O, which is usually constant on a given
machine.

We evaluate the scalability of FlashEigen with the page graph with 3.4 billion
vertices and 129 billion edges. Because the page graph is a directed graph,
its adjacency matrix is asymmetric and we perform singular value decomposition
(SVD) on the adjacency matrix instead of simple eigendecomposition. For the page
graph, we use $2$ for the block size and $2 \times ev$ for the number of blocks
because sparse matrix multiplication is completely bottlenecked by SSDs.
Neither the in-memory eigensolver nor the original Trilinos eigensolver is able
to compute eigenvalues on the page graph with 1TB RAM.

\begin{table}
	\begin{center}
		\small
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			\#eigenvalues & runtime & memory & read & write \\
			\hline
			8 & 4.2 hours & 120GB & 145TB & 4TB \\
			\hline
			32 & 13.3 hours &  &  & \\
			\hline
		\end{tabular}
		\normalsize
	\end{center}
	\caption{The performance and resource consumption of computing eigenvalues
	on the page graph.}
	\label{pg_ev}
\end{table}

FlashEigen computes a fairly large number of eigenvalues within a reasonable
amount of time and consumes a fairly small amount of resources given the large
size of the eigenvalue problem.
The average I/O throughput during the computation of 8 eigenvalues is about
10GB/s, which is very close to the maximal I/O throughput provided by
the SSD array.
Given the relative small memory footprint, we are able to scale FlashEigen
to a much larger eigenvalue problem on our 1TB-RAM machine.

\subsubsection{NMF}

