\section{Experimental Evaluation}

We evaluate the performance of the semi-external memory sparse matrix
multiplication on multiple real-world billion-scale graphs including a web-page
graph with 3.4 billion vertices. We first demonstrate the effectiveness of
both in-memory and external-memory optimizations on sparse matrix multiplication
and then compare the performance of our semi-external memory implementation with
multiple in-memory implementations: \textit{(i)} our in-memory implementation,
\textit{(ii)} MKL and \textit{(iii)} Trilinos. We then evaluate the overall
performance of the applications that require sparse matrix multiplication.
We further demonstrate the scalability of our implementations on a web graph
of 3.4 billion vertices and 129 billion edges.

We conduct all experiments on a non-uniform memory architecture machine with
four Intel Xeon E7-4860 processors, clocked at 2.6 GHz, and 1TB memory of
DDR3-1600. Each processor has 12 cores. The machine has three LSI SAS 9300-8e
host bus adapters (HBA) connected to a SuperMicro storage chassis, in which
24 OCZ Intrepid 3000 SSDs are installed. The 24 SSDs together are capable of
delivering 12 GB/s for read and 10 GB/s for write at maximum. The machine runs
Linux kernel v3.13.0. We use 48 threads for most of experiments by default.

\begin{table}
\begin{center}
\footnotesize
\begin{tabular}{|c|c|c|c|c|}
\hline
Graph datasets & \# Vertices & \# Edges & Directed \\
\hline
Twitter \cite{twitter} & $42$M & $1.5$B & Yes \\
\hline
Friendster \cite{friendster} & $65$M & $1.7$B & No \\
\hline
Page graph \cite{web_graph} & $3.4$B & $129$B & Yes \\
\hline
RMAT-40 \cite{rmat} & 100M & 3.7B & Yes \& No \\
\hline
RMAT-160 \cite{rmat} & 100M & 14B & Yes \& No \\
\hline
\end{tabular}
\normalsize
\end{center}
\caption{Graph data sets.}
\label{graphs}
\end{table}

We use some synthetic graphs and real-world graphs, as shown in Table \ref{graphs}
for evaluation. The smallest graph we use has 42 million vertices and 1.5 billion
edges. The largest graph is the page graph with 3.4 billion vertices
and 129 billion edges, which is two orders of magnitude larger than the smallest
graphs. We generate two synthetic graphs with R-Mat \cite{rmat} to fill
the size gap between the smallest and largest graph. The synthetic graphs
have different density to help evaluate our sparse matrix multiplication more
thoroughly. Some applications in Section \ref{sec:apps} run on directed graphs
and some runs on undirected graphs. As such, we construct a directed and
undirected version for each of the synthetic graphs. The page graph is clustered
by domain, generating good CPU cache hit rates in sparse matrix multiplication.

\subsection{Optimizations on sparse matrix multiplication}
Accelerating SEM SpMM requires both memory and I/O optimizations. To illustrate
the effectiveness of each optimization, we apply the memory and I/O optimizations
incrementally. When only applying memory optimizations to reduce the overhead
of memory access, SEM SpMM may become I/O bound and it may be difficult to
demonstrate their effectiveness. Similarly, it may be difficult to demonstrate
the effectiveness of I/O optimizations when applying them alone. Thus, we first
deply memory optimizations on in-memory sparse matrix multiplication and then
deply I/O optimizations on SEM SpMM with all memory optimizations.

For memory optimizations, we start with an in-memory implementation that
performs sparse matrix multiplication on a sparse matrix in the CSR format
and apply the optimizations incrementally in the following order:
\begin{itemize} \itemsep1pt \parskip0pt \parsep0pt
	\item dispatch partitions of a sparse matrix to threads dynamically for
		computation to balance load (\textit{Load balance}),
	\item partition dense matrices for NUMA (\textit{NUMA}),
	\item organize the non-zero entries in a sparse matrix into tiles to
		increase CPU cache hits (\textit{Cache blocking}),
	\item use CPU vectorization instructions to accelerate arithmetic
		computation (\textit{Vec}),
\end{itemize}

Figure \ref{perf:spmm_opt} shows that almost all of these optimizations have
positive effect on sparse matrix multiplication and all optimizations
together speed up SpMM by $3-5$ times. The degree of effectiveness
varies between different graphs and different numbers of columns in
the dense matrices. \dz{I need to come up better explanations for the speedup.}
%For example, the NUMA optimization is more effective when
%the dense matrices have more columns because more columns in the dense matrices
%require more memory bandwidth. Cache blocking is very effective when
%the dense matrices have fewer columns because it can effectively increase CPU
%cache hits. When there are more columns in the dense matrices, data locality
%improves and the effectiveness of cache blocking becomes less noticeable.
%When there are too many columns, the rows from the input and output matrices
%can no longer be in the CPU cache.
With all optimizations, we have a fast
in-memory sparse matrix dense matrix multiplication, denoted by IM-SpMM.

\begin{figure}
	\begin{center}
		\footnotesize
		\include{SpMM_optimize}
		\caption{The speedup of the in-memory SpMM optimizations on the Friendster
			graph (F) and the Twitter graph (T) for different numbers of
			columns in the dense matrices.}
		\label{perf:spmm_opt}
	\end{center}
\end{figure}

We implement the base version of SEM-SpMM based on the IM-SpMM implementation
above. In this implementation, we write the output dense matrix to SSDs.
We apply the I/O optimizations to SEM SpMM in the following order to show
their effectiveness: \dz{to be complete}
\begin{itemize} \itemsep1pt \parskip0pt \parsep0pt
	\item compact sparse matrix format,
	\item use I/O polling (\textit{polling}),
	\item use a per-thread buffer pool to allocate memory for I/O
		(\textit{buf pool}),
	\item order the portions of the output dense matrix globally and stream them
		to SSDs with large I/O (\textit{global output}).
\end{itemize}

Sparse matrix vector multiplication on some of the graphs requires substantial
I/O throughput and any improvement on I/O improves its overall performance.
Using a compact sparse matrix format accelerates the information retrieval
of the sparse matrix and gives us the best performance boost among all
optimizations. By reducing the number of thread context switches, we can
achieve substantial performance improvement. Memory allocation with
\textit{mmap()} causes significant overhead in high-throughput I/O access,
so we should use a buffer pool to reduce memory allocation when accessing SSDs.

\subsection{The performance of sparse matrix multiplication}

We evaluate the performance of SEM-SpMM and compare its performance with
our IM-SpMM as well as the MKL and Trilinos implementation (Figure
\ref{perf:spmm}). The MKL and Trilinos SpMM cannot run on the page graph.

\subsubsection{The dense matrices can fit in memory}

We first compare the performance of SEM-SpMM against IM-SpMM on all graphs with
the input and output dense matrices stored in memory. In this case, the dense
matrices involved in SpMM have a small number of columns.

\begin{figure}
	\begin{center}
		\footnotesize
		\include{spmm.im.vs.sem}
		\caption{The performance of SEM-SpMM with dense matrices of different
			number of columns, relative to IM-SpMM with dense matrices of
		the same number of columns.}
		\label{perf:spmm_comp}
	\end{center}
\end{figure}

%\begin{figure}
%	\begin{center}
%		\footnotesize
%		\include{spmm.IO}
%		\caption{The I/O throughput generated by SEM-SpMM with different numbers of
%		columns in the dense matrix.}
%		\label{perf:spmm_IO}
%	\end{center}
%\end{figure}

SEM-SpMM gets at least 65-100\% performance of IM-SpMM in all graphs (Figure
\ref{perf:spmm_comp}). Vertex connections in RMAT graphs are generated randomly.
As such, these graphs generate more random memory access than real-world graphs
and thus are more likely to become CPU-bound. The performance gap between
SEM-SpMM and IM-SpMM gets smaller as the number of columns in the dense matrices
increases. Eventually, SEM-SpMM becomes CPU-bound and achieves the same
performance as IM-SpMM. 

\subsubsection{The input dense matrix cannot fit in memory}

We further measure the performance of SEM-SpMM when the input dense matrix
cannot fit in memory. In this experiment, we
measure the performance of multiplying a sparse matrix with a dense matrix
of 32 columns and the input dense matrix is stored on SSDs initially.
We need to load the input dense matrix from SSDs and write the output dense
matrix to SSDs. We perform our SEM-SpMM in this setting with different memory
sizes. We only show the performance result on the Friendster graph because
other graphs have very similar results.

\begin{figure}
	\begin{center}
		\footnotesize
		\include{spmm-32cols}
		\caption{The performance of SEM-SpMM with 32-column matrices on
			the Friendster graph relative to the in-memory implementation.
			SEM-SpMM is broken into multiple SEM-SpMMs based on the memory
			size. We keep only the input dense matrix in memory (in-IM),
			both input and output dense matrices in memory (in-out-IM),
			all matrices in memory (all-IM) to indicate the overhead from SEM.}
		\label{perf:spmm32}
	\end{center}
\end{figure}

As more columns in the input dense matrix can fit in memory, the performance
of SEM-SpMM constantly increases (Figure \ref{perf:spmm32}). When the memory
can fit over four columns, the SEM implementation gets over 50\% of
the in-memory performance. Even when only one column of the input dense matrix
can fit in memory, the SEM implementation can still get 30\% of the in-memory
performance. When the entire input dense matrix can fit in memory, we get about
80\% of the in-memory performance.

Two factors lead to performance loss in SEM-SpMM when the input dense matrix
cannot fit in memory. The main performance loss comes from the loss of data
locality in SpMM when we have to partition the input dense matrix vertically.
To understand
the contribution of this factor, we keep the input dense matrix in memory,
partition it vertically and perform SpMM with the vertical partitions in
the same fashion as semi-external memory. As shown by the line of all-IM
in Figure \ref{perf:spmm32},
partitioning the dense matrix into one-column matrices contributes 60\%
of performance loss. The performance loss caused by this factor decreases
stably when the vertical partition size increases. On the other hand,
the performance loss caused directly by semi-external memory is around 20\%-30\%,
as shown by the line of in-IM and in-out-IM in Figure \ref{perf:spmm32}.
When we can only
keep a small number of columns of the input dense matrix in memory, the overhead
from SEM is mainly caused by streaming the sparse matrix to memory. When we can
keep more columns in memory, the overhead from SEM is mainly caused by reading
the input dense matrix to memory.

\subsubsection{SEM-SpMM vs. other in-memory SpMM}

We compare our SpMM implementation with other state-of-art in-memory SpMM
implementations including the Intel MKL and Trilinos implementations on
natural graphs. The Trilinos implementation can run both in shared memory
and distributed memory, so we measure its performance in our 48-core machine
as well as a EC2 cluster. We run Trilinos in the largest EC2 instances r3.8xlarge
with 16 physical CPU cores and 244GB of RAM each, which is is optimized for
memory-intensive applications.

Both of our IM-SpMM and SEM-SpMM significantly outperform the MKL and Trilinos
implementations on the natural graphs in the 48-core machine (Figure \ref{perf:spmm}).
The Trilinos SpMM is optimized for sparse matrix vector multiplication (SpMV),
but even SEM-SpMV constantly outperforms Trilinos by a factor of $2-3$.
The MKL SpMM performs better when the dense matrices have more columns,
but our SEM-SpMM can still almost twice as fast as MKL when the dense matrix
has eight columns.

\begin{figure}[t!]
	\footnotesize
	\centering
	\begin{subfigure}[b]{0.5\textwidth}
		\centering
		\include{SpMV-awesomer}
		\vspace{-10pt}
		\caption{SpMV}
		\label{perf:spmv}
	\end{subfigure}
	\begin{subfigure}[b]{0.5\textwidth}
		\centering
		\include{SpMM-awesomer}
		\vspace{-10pt}
		\caption{SpMM with a dense matrix of 8 columns.}
		\label{perf:spmm8}
	\end{subfigure}
	\vspace{3pt}
	\caption{The relative performance of different sparse matrix multiplication
		implementations on the 48-core machine. All performance is normalized by
	IM-SpMM for the graphs.}
	\label{perf:spmm}
\end{figure}

\begin{figure}
	\begin{center}
		\footnotesize
		\include{SpMV-EC2}
		\caption{The runtime of our SpMV implementations in our 48-core machine
			compared with Trilinos in EC2 clusters.}
		\label{perf:ec2}
	\end{center}
\end{figure}


\subsection{Performance of the applications}

We evaluate the performance of the implementations of the applications in
Section \ref{sec:apps}. We show the effectiveness of additional memory for
these applications in our implementation and compare their performance
with the state-of-art implementations on smaller graphs. For each application,
we further show their scalability on the page graph.

\subsubsection{PageRank}
We evaluate the performance of our SpMM-based PageRank implementation
(SpMM-PageRank) described
in Section \ref{sec:pagerank}. This implementation requires the input vector
to be in memory, but it is optional to keep the output vector and the degree
vector in memory. We compare the performance of our implementation with
the one in FlashGraph \cite{flashgraph}, which is the fastest to our knowledge,
and the one in GraphLab create, the next generation of
PowerGraph \cite{powergraph}. The PageRank implementation in FlashGraph computes
approximate PageRank values while SpMM-PageRank and GraphLab Create computes
exact PageRank values. We run GraphLab Create completely in memory and run
FlashGraph in semi-external memory.

SpMM-PageRank in memory and in semi-external memory both significantly outperforms
the implementations in FlashGraph and GraphLab Create (Figure \ref{perf:pagerank})
even though FlashGraph computes approximate PageRank and GraphLab Create runs
completely in memory. GraphLab Create is not able to compute PageRank on the Page
graph.

%The experiment results show that although keeping more vectors in memory can
%improve performance for SpMM-PageRank, the performance improvement is insignificant.

\begin{figure}
	\begin{center}
		\footnotesize
		\include{pagerank}
		\caption{The runtime of SpMM-PageRank in 30 iterations. The SEM
			implementation allows to keep different numbers of vectors in memory.
			We compare them with the implementations in FlashGraph and GraphLab
		Create.}
		\label{perf:pagerank}
	\end{center}
\end{figure}

\subsubsection{Eigensolver}

We evaluate the performance of our SEM eigensolver and compare its performance
with our in-memory eigensolver and the Trilinos KrylovSchur eigensolver.
Usually, spectral analysis \cite{} only requires a very small number of
eigenvalues, so we compute eight eigenvalues in this experiment. We perform
eigendecomposition on the smaller undirected graphs
in Table \ref{graphs} and compute singular value decomposition (SVD) on
the Page graph. Only our SEM eigensolver is able to compute eigenvalues
on the page graph. In all experiments, we use one for the block size and
have 16 vectors in the subspace.

\begin{figure}
	\begin{center}
		\footnotesize
		\include{eigen-runtime-8ev}
		\caption{The preformance of our SEM KrylovSchur and the Trilinos KrylovSchur
			eigensolvers relative to our IM KrylovSchur when computing eight
			eigenvalues. SEM-min keeps the entire vector subspace on SSDs and
		SEM-max keeps the entire vector subspace in memory.}
		\label{fig:eigen}
	\end{center}
\end{figure}

For computing eight eigenvalues, our SEM eigensolver achieves at least 40\%
performance of our in-memory eigensolver and has performance comparable to
the Trilinos KrylovSchur eigensolver (Figure \ref{fig:eigen}). When the SEM
eigensolver keeps the entire vector subspace on SSDs, we get the smallest
memory consumption but also the lowest performance. When the SEM eigensolver
can keep the entire vector subspace in memory, it achieves $70\%-80\%$
performance of the in-memory eigensolver, at the cost of more memory consumption.

\subsubsection{NMF}
We evaluate the performance of our NMF implementation on the directed graphs
shown in Table \ref{graphs}. We factorize each of the graphs into two dense
matrices with the rank of 32. We vary the number of columns in the input dense
matrix that can fit in memory.

\begin{figure}
	\begin{center}
		\footnotesize
		\include{NMF}
		\caption{The performance of SEM NMF relative to our in-memory NMF.
			We vary the number of columns in the dense matrices that can fit
		in memory.}
		\label{perf:NMF}
	\end{center}
\end{figure}
