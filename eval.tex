\section{Experimental Evaluation}

We evaluate the performance of the semi-external memory sparse matrix
multiplication on multiple real-world billion-scale graphs including a web-page
graph with 3.4 billion vertices. We first measure the performance of our
semi-external memory implementation and compare it with multiple in-memory
implementations: \textit{(i)} our in-memory implementation, \textit{(ii)} MKL
and \textit{(iii)} Trilinos. We also demonstrate the effectiveness of
CPU and I/O optimizations on sparse matrix multiplication.
We then evaluate the overall performance of the applications in Section
\ref{sec:apps} and demonstrate the impact of the memory size on the applications.

We conduct experiments on a non-uniform memory architecture machine with
four Intel Xeon E7-4860 processors, clocked at 2.6 GHz, and 1TB memory of
DDR3-1600. Each processor has 12 cores. The machine has three LSI SAS 9300-8e
host bus adapters (HBA) connected to a SuperMicro storage chassis, in which
24 OCZ Intrepid 3000 SSDs are installed. The 24 SSDs together are capable of
delivering 12 GB/s for read and 10 GB/s for write at maximum. The machine runs
Linux kernel v3.13.0. We use 48 threads for our in-memory and semi-external
implementation.

\begin{table}
\begin{center}
\footnotesize
\begin{tabular}{|c|c|c|c|c|}
\hline
Graph datasets & \# Vertices & \# Edges & Directed \\
\hline
Twitter \cite{twitter} & $42$M & $1.5$B & Yes \\
\hline
Friendster \cite{friendster} & $65$M & $1.7$B & No \\
\hline
Page graph \cite{web_graph} & $3.4$B & $129$B & Yes \\
\hline
RMAT-40 \cite{rmat} & 100M & 3.7B & Yes \& No \\
\hline
RMAT-160 \cite{rmat} & 100M & 14B & Yes \& No \\
\hline
\end{tabular}
\normalsize
\end{center}
\caption{Graph data sets.}
\label{graphs}
\end{table}

We use the adjacency matrices of the graphs in Table \ref{graphs} for performance
evaluation. The smallest graph we use has 42 million vertices and 1.5 billion
edges. The largest graph is the Page graph with 3.4 billion vertices
and 129 billion edges, which is two orders of magnitude larger than the smallest
graphs. We generate two synthetic graphs with R-Mat \cite{rmat} to fill the size
gap between the smallest and largest graph. We construct a directed and
undirected version for each of the synthetic graphs
because some applications in Section \ref{sec:apps} run on directed graphs
and some run on undirected graphs. We always use the undirected version for
performance evaluation of sparse matrix multiplication. The Page graph is clustered
by domain, generating good CPU cache hit rates in sparse matrix multiplication.

\subsection{The performance of sparse matrix multiplication}

We evaluate the performance of our semi-external memory sparse matrix
multiplication (SEM-SpMM) and compare its performance with our in-memory sparse
matrix multiplication (IM-SpMM) as well as other state-of-art in-memory sparse
matrix multiplication implementations including the ones in Intel MKL and Trilinos
on the graphs in Table \ref{graphs}. The MKL and Trilinos implementations
cannot run on the Page graph.

\subsubsection{SEM-SpMM vs. IM-SpMM}

\begin{figure}
	\begin{center}
		\footnotesize
		\include{spmm.im.vs.sem}
		\caption{The performance of SEM-SpMM with dense matrices of different
			numbers of columns, relative to IM-SpMM for the dense matrix with
			the same number of columns.}
		\label{perf:spmm_comp}
	\end{center}
\end{figure}

We first compare the performance of SEM-SpMM against IM-SpMM on all graphs with
the input and output dense matrices stored in memory. In this case, the dense
matrices involved in SpMM have a small number of columns.

SEM-SpMM gets 65-100\% performance of IM-SpMM in all graphs (Figure
\ref{perf:spmm_comp}). The performance gap between IM-SpMM and SEM-SpMM
is affected by randomness of vertex connection. The gap is smaller if
vertex connection in a graph is more random. The Page graph is relatively
well clustered, so SpMM on this graph is less CPU-bound than others.
Even for the Page graph, SEM-SpMM gets 65\% performance of IM-SpMM.
The other factor of affecting the performance gap is the number of columns
in the dense matrices. The gap gets smaller as the number of columns in
the dense matrices increases. For all graphs, SEM-SpMM requires a very small
number of columns to become CPU-bound and achieve performance close to IM-SpMM.

\subsubsection{SEM-SpMM vs. other in-memory SpMM}
In this section, we compare SEM-SpMM with the Intel MKL and Trilinos
implementations. Intel MKL runs on shared-memory machines. Trilinos can run in
both shared memory and distributed memory, so we measure its performance in
our 48-core NUMA machine as well as an EC2 cluster. We run Trilinos in the largest
EC2 instances r3.8xlarge, where each has 16 physical CPU cores and 244GB of RAM
and is optimized for memory-intensive applications. The EC2 instances are
connected with 10Gbps network.

\begin{figure}
	\footnotesize
	\centering
	\begin{subfigure}[b]{0.5\textwidth}
		\centering
		\include{SpMV-awesomer}
		\vspace{-10pt}
		\caption{SpMV}
		\label{perf:spmv}
	\end{subfigure}
	\begin{subfigure}[b]{0.5\textwidth}
		\centering
		\include{SpMM-awesomer}
		\vspace{-10pt}
		\caption{SpMM with a dense matrix of 8 columns.}
		\label{perf:spmm8}
	\end{subfigure}
	\vspace{3pt}
	\caption{The performance of different sparse matrix multiplication
		implementations on the 48-core machine relative to IM-SpMM for
	the same graphs.}
	\label{perf:spmm}
\end{figure}

Our SEM-SpMM significantly outperforms Intel MKL and Trilinos on the natural
graphs on our NUMA machine (Figure \ref{perf:spmm}). In this case, we compare
performance of our SEM-SpMM with Intel MKL and Trilinos for both sparse matrix
vector multiplication (SpMV) and sparse matrix dense matrix multiplication (SpMM).
The Trilinos implementation is optimized for SpMV. Our SEM-SpMM still
constantly outperforms Trilinos by a factor of $2-3$ even for SpMV. The MKL
implementation has better optimizations for SpMM, but our SEM-SpMM can still
almost twice as fast as MKL when the dense matrix has eight columns.

SEM-SpMM only consumes a small fraction of memory compared with IM-SpMM and
other SpMM implementations (Figure \ref{perf:spmm_mem}). SEM-SpMM consumes
memory for the input dense matrix as well as per-thread local memory buffers
for the sparse matrix and the output dense matrix. When we use 48 threads for
SpMM, the memory used by local memory buffers in each thread is significant
but is relatively constant for different graph sizes. As such, we only show
the memory consumption on the largest graph RMAT-160 in Figure \ref{perf:spmm}.
Despite considerable memory consumed by
local memory buffers for SEM-SpMM, SEM-SpMM uses about one tenth of the memory
used by IM-SpMM. We also observe that IM-SpMM consumes much less memory than
MKL and Trilinos thanks to the very compact format for a sparse matrix.

\begin{figure}
	\begin{center}
		\footnotesize
		\include{SpMM-mem}
		\caption{Memory consumption of different SpMM implementations on
		RMAT-160.}
		\label{perf:spmm_mem}
	\end{center}
\end{figure}

\begin{figure}
	\begin{center}
		\footnotesize
		\include{SpMV-EC2}
		\caption{The performance of our SEM-SpMV in our 48-core machine
		and Trilinos in EC2 clusters relative to IM-SpMV for the same graph.}
		\label{perf:ec2}
	\end{center}
\end{figure}

Our SEM-SpMM also outperforms Trilinos that runs in the Amazon cloud by a large
margin for SpMV, especially on real-world graphs (Figure \ref{perf:ec2}).
When Trilinos runs on 8 EC2 instances, it has 2.5 times as many CPU cores as
our SEM-SpMM on the NUMA machine. We do not compare SEM-SpMM with Trilinos
for SpMM because Trilinos is not optimized for SpMM as shown above. Trilinos
is not able to run SpMV on RMAT-160 on two EC2 nodes.
One of the main reasons that our SEM-SpMM performs much
better on real-world graphs is that these graphs are more likely to cause
load imbalance and our SEM-SpMM is able to balance load much better than
Trilinos.

\subsubsection{SEM-SpMM with a large input dense matrix}

We further measure the performance of SEM-SpMM with a large input dense matrix
that cannot fit in memory. In this experiment, we measure the performance of
multiplying a sparse matrix with a dense matrix of 32 columns and the input
dense matrix is stored on SSDs initially. We study the impact of memory size
on the performance of SEM-SpMM by artificially varying the number of columns
that can fit in memory. In each run, we need to load the input dense matrix from
SSDs and stream the output dense matrix to SSDs. We do not show the result on
the Page graph because the dense matrix with 32 columns for the Page graph
cannot fit in memory.

\begin{figure}
	\begin{center}
		\footnotesize
		\include{spmm-32cols}
		\caption{The performance of SEM-SpMM with 32-column matrices
			relative to IM-SpMM. SEM-SpMM is broken into multiple SEM-SpMMs
		based on the memory size.}
		\label{perf:spmm32}
	\end{center}
\end{figure}

As more columns in the input dense matrix can fit in memory, the performance
of SEM-SpMM constantly increases (Figure \ref{perf:spmm32}). When the memory
can fit over four columns of the input dense matrix, SEM-SpMM gets over 50\%
of the performance of IM-SpMM. Even when only one column of the input dense
matrix can fit in memory, SEM-SpMM still gets 30\% of the in-memory
performance. When the entire input dense matrix can fit in memory, we get over
80\% of the in-memory performance.

\begin{figure}
	\begin{center}
		\footnotesize
		\include{spmm-32cols-overhead}
		\caption{The overhead breakdown of SEM-SpMM on the Friendster
			graph with a dense matrix of 32 columns when the number
			of columns that fit in memory varies. }
		\label{perf:spmm32_over}
	\end{center}
\end{figure}

Two main factors lead to performance loss in SEM-SpMM when the input dense matrix
cannot fit in memory. We illustrate the contribution of four potential overheads
in SEM-SpMM on the Friendster graph (Figure \ref{perf:spmm32_over}). The main
performance loss comes from the loss of data locality in SpMM when we have to
partition the input dense matrix vertically.
Partitioning the dense matrix into one-column matrices contributes 60\%
of performance loss. The performance loss caused by this factor drops quickly
when the vertical partition size increases. Keeping the sparse matrix on SSDs
also contributes some performance loss when the dense matrix is partitioned
into small matrices. The overhead almost goes away when more than four columns
of the dense matrix can fit in memory. The overhead of streaming the output dense
matrix to SSDs and reading the input dense matrix to memory is less significant.

\subsection{Optimizations on sparse matrix multiplication}
Accelerating SEM-SpMM requires both computation and I/O optimizations. Due to
the limit of space, we only illustrate the effectiveness of computation
optimizations.
%Thus, we first deply computation optimizations
%on in-memory sparse matrix multiplication and then
%deply I/O optimizations on SEM-SpMM with all computation optimizations.

We deploy many computation optimizations shown in Section \ref{sec:spmm} and
here we illustrate some major optimizations. We start with an in-memory
implementation that
performs sparse matrix multiplication on a sparse matrix in the CSR format
and apply the optimizations incrementally in the following order:
\begin{itemize} \itemsep1pt \parskip0pt \parsep0pt
	\item dispatch partitions of a sparse matrix to threads dynamically
		to balance load (\textit{Load balance}),
	\item partition dense matrices for NUMA (\textit{NUMA}),
	\item organize the non-zero entries in a sparse matrix into tiles to
		increase CPU cache hits (\textit{Cache blocking}),
	\item use CPU vectorization instructions to accelerate arithmetic
		computation (\textit{Vec}),
\end{itemize}

All of these optimizations have positive effect on sparse matrix
multiplication and all optimizations together speed up SpMM by $3-5$ times
(Figure \ref{perf:spmm_opt}). The degree of effectiveness
varies between different graphs and different numbers of columns in
the dense matrices. The largest performance boost is from cache blocking,
especially for SpMV.
This is expected because the main overhead of SpMV comes from random memory
access and cache blocking significantly increases CPU cache hits to reduce
random memory access. CPU vectorization is only effective on SpMM because
it optimizes computation on a row of the dense matrix.
%For example, the NUMA optimization is more effective when
%the dense matrices have more columns because more columns in the dense
%matrices require more memory bandwidth. Cache blocking is very effective when
%the dense matrices have fewer columns because it can effectively increase CPU
%cache hits. When there are more columns in the dense matrices, data locality
%improves and the effectiveness of cache blocking becomes less noticeable.
%When there are too many columns, the rows from the input and output matrices
%can no longer be in the CPU cache.
With all optimizations, we have a fast in-memory implementation for both
sparse matrix vector multiplication and sparse matrix dense matrix multiplication.
We denote this implementation with IM-SpMM.

\begin{figure}
	\begin{center}
		\footnotesize
		\include{SpMM_optimize}
		\caption{The speedup of computation optimizations for SpMM on the Friendster
			graph (F) and the Twitter graph (T) for different numbers of
			columns in the dense matrices.}
		\label{perf:spmm_opt}
	\end{center}
\end{figure}

%We implement the base version of SEM-SpMM based on the IM-SpMM implementation
%above. In this implementation, we write the output dense matrix to SSDs.
%SpMM on a dense matrix with a few columns is usually CPU-bound. As such,
%we illustrate the I/O optimizations on the base SEM-SpMV in the following order to show
%their effectiveness:
%\begin{itemize} \itemsep1pt \parskip0pt \parsep0pt
%	\item use a compact sparse matrix format to reduce the number of bits read
%		from SSDs,
%	\item reduce memory allocation overhead with \textit{mmap()} by maintaining
%		a per-thread buffer pool to allocate memory for I/O (\textit{buf pool}),
%	\item reduce the number of thread context switch for I/O access with I/O
%		polling (\textit{polling}),
%	\item order the portions of the output dense matrix globally and stream them
%		to SSDs with large I/O (\textit{global output}).
%\end{itemize}

%\begin{figure}
%	\begin{center}
%		\footnotesize
%		\include{SpMM_opts_io}
%		\caption{The speedup of I/O optimizations for SpMV on real-world graphs.}
%		\label{perf:spmm_opt_io}
%	\end{center}
%\end{figure}

%SEM-SpMV on some of the graphs requires substantial I/O throughput and
%any improvement on I/O leads to overall performance boost (Figure
%\ref{perf:spmm_opt_io}). The Page graph is relatively well clustered, so
%SEM-SpMV on this graph is less computationally intensive than other graphs
%and requires higher I/O throughput.
%Using a compact sparse matrix format accelerates the information retrieval
%of the sparse matrix and gives us the best performance boost among all
%optimizations. Reducing the overhead of memory allocation with \textit{mmap()}
%gives significant performance boost. Reducing the number of thread context
%switches with polling achieve substantial performance improvement. 

\subsection{Performance of the applications}

We evaluate the performance of the implementations of the applications in
Section \ref{sec:apps}. We show the effectiveness of additional memory for
these applications in our implementation and compare their performance
with the state-of-art implementations on smaller graphs.

\subsubsection{PageRank}
We evaluate the performance of our SpMM-based PageRank implementation
(SpMM-PageRank) described
in Section \ref{sec:pagerank}. This implementation requires the input vector
to be in memory, but it is optional to keep the output vector and the degree
vector in memory. We compare the performance of our implementation with
the one in FlashGraph \cite{flashgraph}, which is the fastest to our knowledge,
and the one in GraphLab create, the next generation of
PowerGraph \cite{powergraph}. The PageRank implementation in FlashGraph computes
approximate PageRank values while SpMM-PageRank and GraphLab Create computes
exact PageRank values. We run GraphLab Create completely in memory and run
FlashGraph in semi-external memory. GraphLab Create is not able to compute
PageRank on the Page graph.

SpMM-PageRank in memory and in semi-external memory both significantly outperforms
the implementations in FlashGraph and GraphLab Create (Figure \ref{perf:pagerank})
even though FlashGraph computes approximate PageRank and GraphLab Create runs
completely in memory. The main computation of PageRank is to access PageRank
values from neighbor vertices, which is essentially the same computation in
sparse matrix vector multiplication. Our SpMM is highly optimized for both CPU
and I/O. Even though SpMM-PageRank performs more computation than FlashGraph,
it performs the computation required by PageRank much more efficiently and
reads less data from SSDs than FlashGraph. SpMM-PageRank and the implementation
in GraphLab create performs the same computation, but SpMM-PageRank
performs the computation much more efficiently.

The experiment results also show that keeping more vectors in memory has modest
performance improvement for SpMM-PageRank. As such, SpMM-PageRank only needs
to keep one vector in memory, which results in very small memory consumption.

\begin{figure}
	\begin{center}
		\footnotesize
		\include{pagerank}
		\caption{The runtime of SpMM-PageRank in 30 iterations. The SEM
			implementation allows to keep different numbers of vectors in memory.
			We compare them with the implementations in FlashGraph and GraphLab
		Create.}
		\label{perf:pagerank}
	\end{center}
\end{figure}

\subsubsection{Eigensolver}

We evaluate the performance of our SEM KrylovSchur eigensolver and compare
its performance
with our in-memory eigensolver and the Trilinos KrylovSchur eigensolver.
Usually, spectral analysis \cite{} only requires a very small number of
eigenvalues, so we compute eight eigenvalues in this experiment. We run
the eigensolvers on the smaller undirected graphs
in Table \ref{graphs}. To evaluate the scalability of the SEM eigensolver,
we compute singular value decomposition (SVD) on the Page graph. Among all of
the eigensolvers, only our SEM eigensolver is able to compute eigenvalues
on the Page graph.

\begin{figure}
	\begin{center}
		\footnotesize
		\include{eigen-runtime-8ev}
		\caption{The preformance of our SEM KrylovSchur, our in-memory eigensolver
			and the Trilinos eigensolvers when computing eight
			eigenvalues. SEM-min keeps the entire vector subspace on SSDs and
		SEM-max keeps the entire vector subspace in memory.}
		\label{fig:eigen}
	\end{center}
\end{figure}

For computing eight eigenvalues, our SEM eigensolver achieves performance
comparable to our in-memory eigensolver and the Trilinos eigensolver
and can scale to very large graphs (Figure \ref{fig:eigen}).
Unlike PageRank, an eigensolver has many more vector or dense matrix operations.
As such, the memory size has noticeable impact on performance.
For the setting with the minimum memory consumption, it has at least 45\%
performance of our in-memory eigensolver; when keeping the entire subspace
in memory, it has almost the same performance as our in-memory eigensolver.

\subsubsection{NMF}
We evaluate the performance of our NMF implementation (SEM-NMF) on the directed
graphs in Table \ref{graphs}. The dense matrices for NMF can be as large as
the sparse matrix. As such, we experiment the effect of the memory size on
the performance of SEM-NMF by varying the number of columns in the dense matrix
that can fit in memory. We also compare the performance of SEM-NMF with
a high-performance NMF implementation SmallK \cite{SmallK}, built on top of
the numeric library Elemental \cite{elemental}. We factorize
each of the graphs into two $n \times k$ non-negative dense matrices and
we use $k=16$ because $16$ is the largest $k$ that SmallK supports for
the graphs in Table \ref{graphs}.

\begin{figure}
	\begin{center}
		\footnotesize
		\include{NMF}
		\caption{The runtime per iteration of SEM-NMF on directed graphs.
			We vary the number of columns in the dense matrices that are kept
		in memory to evaluate the memory size on the performance of SEM-NMF.}
		\label{perf:NMF}
	\end{center}
\end{figure}

We can significantly improve the performance of SEM-NMF by keeping more columns
of the input dense matrix in memory (Figure \ref{perf:NMF}). The performance
improvement is more significant when the number of columns that fit in memory
is small. When we can keep eight columns of the input dense matrix in memory,
SEM-NMF achieves over 60\% of the performance of the in-memory implementation.

SEM-NMF significantly outperforms other NMF implementations in the literature.
SmallK is the closest competitor in the literature. We run the same NMF algorithm
in SmallK. As shown in Figure \ref{perf:NMF}, SEM-NMF outperforms SmallK by
a large factor on all graphs. There are many MapReduce implementations in
the literature \cite{Liao14, Yin14, Liu10}. They run on sparse
matrices with tens of millions of non-zero entries but generally take
one or two orders of magnitude more time than our SEM-NMF on the sparse matrices
with billions or even tens of billions of non-zero entries.
