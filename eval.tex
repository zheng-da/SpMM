\section{Experimental Evaluation}

We evaluate the performance of the semi-external memory sparse matrix
multiplication on multiple real-world billion-scale graphs including a web-page
graph with 3.4 billion vertices. We first demonstrate the effectiveness of
both in-memory and external-memory optimizations on sparse matrix multiplication
and then compare the performance of our semi-external memory implementation with
multiple in-memory implementations: \textit{(i)} our in-memory implementation,
\textit{(ii)} MKL and \textit{(iii)} Trilinos. We then evaluate the overall
performance of the applications that require sparse matrix multiplication.
We further demonstrate the scalability of our implementations on a web graph
of 3.4 billion vertices and 129 billion edges.

We conduct all experiments on a non-uniform memory architecture machine with
four Intel Xeon E7-4860 processors, clocked at 2.6 GHz, and 1TB memory of
DDR3-1600. Each processor has 12 cores. The machine has three LSI SAS 9300-8e
host bus adapters (HBA) connected to a SuperMicro storage chassis, in which
24 OCZ Intrepid 3000 SSDs are installed. The 24 SSDs together are capable of
delivering 12 GB/s for read and 10 GB/s for write at maximum. The machine runs
Linux kernel v3.13.0. We use 48 threads for most of experiments by default.

\begin{table}
\begin{center}
\footnotesize
\begin{tabular}{|c|c|c|c|c|}
\hline
Graph datasets & \# Vertices & \# Edges & Directed \\
\hline
Twitter \cite{twitter} & $42$M & $1.5$B & Yes \\
\hline
Friendster \cite{friendster} & $65$M & $1.7$B & No \\
\hline
Page graph \cite{web_graph} & $3.4$B & $129$B & Yes \\
\hline
RMAT-40 \cite{rmat} & 100M & 3.7B & No \\
\hline
RMAT-160 \cite{rmat} & 100M & 14B & No \\
\hline
\end{tabular}
\normalsize
\end{center}
\caption{Graph data sets.}
\label{graphs}
\end{table}

We use some synthetic graphs and real-world graphs, as shown in Table \ref{graphs}
for evaluation. The smallest graph we use has 42 million vertices and 1.5 billion
edges. The largest graph is the page graph with 3.4 billion vertices
and 129 billion edges, which is two orders of magnitude larger than the smallest
graphs. We generate two synthetic graphs with R-Mat \cite{rmat} to fill
the size gap between the smallest and largest graph. The synthetic graphs
have different density to help evaluate our sparse matrix multiplication more
thoroughly. Some applications in Section \ref{sec:apps} run on directed graphs
and some runs on undirected graphs. As such, we construct a directed and
undirected version for each of the synthetic graphs. The page graph is clustered
by domain, generating good CPU cache hit rates in sparse matrix multiplication.

\subsection{Optimizations on sparse matrix multiplication}
We first deploy a set of in-memory optimizations to implement a fast
in-memory sparse matrix multiplication and then deploy a set of I/O
optimizations to further speed up this operation in semi-external memory.
We apply the memory and I/O optimizations incrementally to illustrate
the effectiveness of each optimization.

For memory optimizations, we start with an implementation that performs sparse
matrix multiplication on a sparse matrix in the CSR format and apply
the optimizations incrementally in the following order:
\begin{itemize} \itemsep1pt \parskip0pt \parsep0pt
	\item load balancing \dz{to be complete}
	\item partition dense matrices for NUMA (\textit{NUMA}),
	\item partition the sparse matrix in both dimensions into tiles of
		$16K \times 16K$ (\textit{Cache blocking}),
	\item organize multiple physical tiles into a super tile to fill CPU cache
		(\textit{Super tile}),
	\item use CPU vectorization instructions (\textit{Vec}),
	\item allocate a local buffer to store the intermediate multiplication
		result on tiles(\textit{Local write}),
	\item combine the SCSR and COO format to reduce the number of conditional
		jump CPU instructions (\textit{SCSR + COO}),
\end{itemize}

Figure \ref{perf:spmm_opt} shows that almost all of these optimizations have
positive effect on sparse matrix multiplication and all optimizations
together speed up the operation by $2-4$ times. The degree of effectiveness
varies between different graphs and different numbers of columns in
the dense matrices. For example, the NUMA optimization is more effective when
the dense matrices have more columns because more columns in the dense matrices
require more memory bandwidth. Cache blocking is very effective when
the dense matrices have fewer columns because it can effectively increase CPU
cache hits. When there are more columns in the dense matrices, data locality
improves and the effectiveness of cache blocking becomes less noticeable.
When there are too many columns, the rows from
the input and output matrices can no longer be in the CPU cache. Thus, it even
has a little negative effect on the Friendster graph when the dense matrices
have $16$ columns. With all optimizations, we have a fast in-memory sparse
matrix dense matrix multiplication, denoted by IM-SpMM.

\begin{figure}
	\begin{center}
		\footnotesize
		\include{SpMM_optimize}
		\caption{The effectiveness of the SpMM optimizations on the Friendster
			graph (F) and the Twitter graph (T) for different numbers of
			columns in the dense matrices.}
		\label{perf:spmm_opt}
	\end{center}
\end{figure}

We modify the IM-SpMM implementation above for the semi-external memory sparse
matrix multiplication (SEM-SpMM) on SSDs and write the output dense matrix to
SSDs. We apply the I/O optimizations to sparse matrix vector multiplication
in the following order to show their effectiveness: \dz{to be complete}
\begin{itemize} \itemsep1pt \parskip0pt \parsep0pt
	\item compress a sparse matrix,
	\item use I/O polling (\textit{polling}),
	\item use a per-thread buffer pool to allocate memory for I/O
		(\textit{buf pool}),
	\item order the portions of the output dense matrix globally and stream them
		to SSDs with large I/O (\textit{global output}).
\end{itemize}

Sparse matrix vector multiplication requires substantial I/O throughput and
any improvement on I/O improves the overall performance of this operation.
Compressing the sparse matrix helps us to accelerate the information retrieval
of the sparse matrix and gives us the best performance boost among all
optimizations. By reducing the number of thread context switches, we can
achieve substantial performance improvement. Memory allocation with
\textit{mmap()} causes significant overhead in high-throughput I/O access,
so we should use a buffer pool to reduce memory allocation when accessing SSDs.

\subsection{The performance of sparse matrix multiplication}

We evaluate the performance of SEM-SpMM and compare its performance with
our in-memory implementation (IM-SpMM) as well as the MKL and Trilinos
implementation (Figure \ref{perf:spmm}). The MKL and Trilinos SpMM cannot run
on the page graph.

\subsubsection{The dense matrices can fit in memory}

We first compare the performance of SEM-SpMM against IM-SpMM on all graphs with
the input and output dense matrices stored in memory. In this case, the dense
matrices involved in SpMM have a small number of columns.

\begin{figure}
	\begin{center}
		\footnotesize
		\include{spmm.im.vs.sem}
		\caption{The performance of SEM-SpMM with different numbers of columns
			in the dense matrix on graphs in Table \ref{graphs} relative to IM-SpMM.}
		\label{perf:spmm_comp}
	\end{center}
\end{figure}

\begin{figure}
	\begin{center}
		\footnotesize
		\include{spmm.IO}
		\caption{The I/O throughput generated by SEM-SpMM with different numbers of
		columns in the dense matrix.}
		\label{perf:spmm_IO}
	\end{center}
\end{figure}

SEM-SpMM gets at least 60-100\% performance of IM-SpMM in all graphs (Figure
\ref{perf:spmm_comp}). The performance gap becomes smaller for all graphs as
the number of columns in the dense matrices increases.
We show the average I/O throughput generated by SEM-SpMM in
Figure \ref{perf:spmm_IO}. Sparse matrix vector multiplication generates very high I/O
throughput. For the page graph, the I/O throughput gets close to the bandwidth
of the SSD array when the number of columns in the dense matrices is small,
which suggests that I/O be the bottleneck. As the number of columns increases,
the I/O throughput caused by SEM-SpMM drops and CPU becomes the bottleneck of
the system.
\dz{can we show that we have either saturate CPU or SSDs or both?}

When comparing the performance of our SpMM implementations with state-of-art
in-memory SpMM implementations, both IM-SpMM and SEM-SpMM significantly
outperform the MKL and Trilinos implementations on the natural graphs (Figure
\ref{perf:spmm}). The performance of different implementations
of sparse matrix multiplication with different numbers of columns in the dense
matrices on the Friendster graph. The performance result is very similar
on other graphs. The Trilinos SpMM is optimized for sparse matrix vector
multiplication (SpMV). But even for SpMV, SEM-SpMV outperforms Trilinos by
almost 30\%. The MKL SpMM performs better when the dense matrices have more columns,
but our implementations can still outperform MKL by a factor of $2-3$ in most
settings.
\dz{I should add another line for distributed Trilinos.}

\begin{figure}
	\begin{center}
		\footnotesize
		\include{SpMM-Friendster}
		\caption{The runtime of IM-SpMM, SEM-SpMM, the MKL and the Trilinos
			implementations on the Friendster graph.}
		\label{perf:spmm}
	\end{center}
\end{figure}

\subsubsection{The input dense matrix cannot fit in memory}

We further measure the performance of SEM-SpMM when the input dense matrix
cannot fit in memory. In this experiment, we
measure the performance of multiplying a sparse matrix with a dense matrix
of 32 columns and the input dense matrix is stored on SSDs initially.
We need to load the input dense matrix from SSDs and write the output dense
matrix to SSDs. We perform our SEM-SpMM in this setting with different memory
sizes. We only show the performance result on the Friendster graph because
other graphs have very similar results.

\begin{figure}
	\begin{center}
		\footnotesize
		\include{spmm-32cols}
		\caption{The performance of SEM-SpMM with 32-column matrices on
			the Friendster graph relative to the in-memory implementation.
			SEM-SpMM is broken into multiple SEM-SpMMs based on the memory
			size. We keep only the input dense matrix in memory (in-IM),
			both input and output dense matrices in memory (in-out-IM),
			all matrices in memory (all-IM) to indicate the overhead from SEM.}
		\label{perf:spmm32}
	\end{center}
\end{figure}

As more columns in the input dense matrix can fit in memory, the performance
of SEM-SpMM constantly increases (Figure \ref{perf:spmm32}). When the memory
can fit over four columns, the SEM implementation gets over 50\% of
the in-memory performance. Even when only one column of the input dense matrix
can fit in memory, the SEM implementation can still get 30\% of the in-memory
performance. When the entire input dense matrix can fit in memory, we get about
80\% of the in-memory performance.

Two factors lead to performance loss in SEM-SpMM when the input dense matrix
cannot fit in memory. The main performance loss comes from the loss of data
locality in SpMM when we have to partition the input dense matrix vertically.
To understand
the contribution of this factor, we keep the input dense matrix in memory,
partition it vertically and perform SpMM with the vertical partitions in
the same fashion as semi-external memory. As shown by the line of all-IM
in Figure \ref{perf:spmm32},
partitioning the dense matrix into one-column matrices contributes 60\%
of performance loss. The performance loss caused by this factor decreases
stably when the vertical partition size increases. On the other hand,
the performance loss caused directly by semi-external memory is around 20\%-30\%,
as shown by the line of in-IM and in-out-IM in Figure \ref{perf:spmm32}.
When we can only
keep a small number of columns of the input dense matrix in memory, the overhead
from SEM is mainly caused by streaming the sparse matrix to memory. When we can
keep more columns in memory, the overhead from SEM is mainly caused by reading
the input dense matrix to memory.

\subsection{Performance of the applications}

We evaluate the performance of the implementations of the applications in
Section \ref{sec:apps}. We compare the performance of our implementations
with the state-of-art implementations on smaller graphs. For each application,
we further show their scalability on the page graph.

\subsubsection{PageRank}
We evaluate the performance of our SpMM-based PageRank implementation
(SpMM-PageRank) described
in Section \ref{sec:pagerank}. This implementation requires the input vector
to be in memory, but it is optional to keep the output vector and the degree
vector in memory. We compare the performance of our implementation with
the one in FlashGraph \cite{flashgraph}, which is the fastest to our knowledge,
and the one in GraphLab create \cite{graphlab_create}, the next generation of
PowerGraph \cite{powergraph}. The PageRank implementation in FlashGraph computes
approximate PageRank values while SpMM-PageRank and GraphLab Create computes
exact PageRank values. We run GraphLab Create completely in memory and run
FlashGraph in semi-external memory.

SpMM-PageRank in memory and in semi-external memory both significantly outperforms
the implementations in FlashGraph and GraphLab Create (Figure \ref{perf:pagerank})
even though FlashGraph computes approximate PageRank and GraphLab Create runs
completely in memory. GraphLab Create is not able to compute PageRank on the Page
graph.

The experiment results show that although keeping more vectors in memory can
improve performance for SpMM-PageRank, the performance improvement is insignificant.

\begin{figure}
	\begin{center}
		\footnotesize
		\include{pagerank}
		\caption{The runtime of SpMM-PageRank in 30 iterations. The SEM
			implementation allows to keep different numbers of vectors in memory.
			We compare them with the implementations in FlashGraph and GraphLab
		Create.}
		\label{perf:pagerank}
	\end{center}
\end{figure}

\subsubsection{Eigensolver}

We evaluate the performance of our SEM eigensolver and compare its performance
with our in-memory eigensolver and the Trilinos KrylovSchur eigensolver.
We compute a small number of eigenvalues on the smaller undirected graphs
in Table \ref{graphs} and compute singular value decomposition (SVD) on
the Page graph. Only our SEM eigensolver is able to compute eigenvalues
on the page graph.

\begin{figure}
	\begin{center}
		\footnotesize
		\include{eigen-runtime-8ev}
		\caption{The performance of the Trilinos KrylovSchur and our SEM KrylovSchur
		relative to the our in-memory KrylovSchur when computing eight eigenvalues.}
		\label{fig:eigen}
	\end{center}
\end{figure}

When computing eight eigenvalues, our SEM eigensolver achieves at least 40\%
performance of our in-memory eigensolver and has performance comparable to
the Trilinos KrylovSchur eigensolver (Figure \ref{fig:eigen}).
In this experiment, the block size is one and we only keep two vectors
(the input and output vectors for SpMV) in memory and store the entire
vector subspace on SSDs.

\dz{We need an experiment to show how well additional memory can help improve
performance.}

We evaluate the scalability of the SEM eigensolver with the page graph with 3.4 billion
vertices and 129 billion edges. Because the page graph is a directed graph,
its adjacency matrix is asymmetric and we perform singular value decomposition
(SVD) on the adjacency matrix instead of simple eigendecomposition. For the page
graph, we use $2$ for the block size and $2 \times ev$ for the number of blocks
because sparse matrix vector multiplication is bottlenecked by SSDs.
Neither the in-memory eigensolver nor the Trilinos eigensolver is able
to compute eigenvalues on the page graph with 1TB RAM.

\begin{table}
	\begin{center}
		\small
		\begin{tabular}{|c|c|c|c|c|}
			\hline
			\#eigenvalues & runtime & memory & read & write \\
			\hline
			8 & 4.2 hours & 120GB & 145TB & 4TB \\
			\hline
		\end{tabular}
		\normalsize
	\end{center}
	\caption{The performance and resource consumption of computing eigenvalues
	on the page graph.}
	\label{pg_ev}
\end{table}

%The SEM eigensolver computes a fairly large number of eigenvalues within a reasonable
%amount of time and consumes a fairly small amount of resources given the large
%size of the eigenvalue problem.
%The average I/O throughput during the computation of 8 eigenvalues is about
%10GB/s, which is very close to the maximal I/O throughput provided by
%the SSD array. The relative small memory footprint suggests that the SEM
%eigensolver is able to scale to a much larger eigenvalue problem on our
%1TB-RAM machine.

\subsubsection{NMF}
We evaluate the performance of our NMF implementation on the directed graphs
shown in Table \ref{graphs}. We factorize each of the graphs into two dense
matrices with the rank of 32. We vary the number of columns in the input dense
matrix that can fit in memory.

\begin{figure}
	\begin{center}
		\footnotesize
		\include{NMF}
		\caption{The runtime of NMF in one iteration.}
		\label{perf:NMF}
	\end{center}
\end{figure}
