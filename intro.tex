% What problem are you going to solve.
Sparse matrix multiplication is an important computation with a wide variety
of applications in scientific computing, machine learning, and graph analysis.
For example, matrix factorization algorithms on a sparse matrix, such as
singular value decomposition (SVD) \cite{svd} and non-negative matrix
factorization (NMF) \cite{nmf}, require sparse matrix multiplication.
Graph analysis algorithms such as PageRank \cite{pagerank} can be
formulated as sparse matrix multiplication or generalized sparse matrix
multiplication \cite{Mattson13}. Some of
the algorithms, such as PageRank and SVD, require sparse matrix vector
multiplication. Others, such as NMF, require sparse matrix dense
matrix multiplication.
% Why is it hard?

It is challenging to implement sparse matrix multiplication efficiently. 
It has very low computation density and its
performance is limited by memory access. Sparse matrix multiplication usually achieves only
a small fraction of the peak performance of a modern processor \cite{Williams07}.

Sparse matrices derived from graphs impose additional challenges.
Many graphs have a power-law distribution in vertex
degree.  This produces a power-law distribution in the number of non-zero 
entries per column or row in the associated matrix, which   
results in load imbalance in parallel decompositions of sparse 
matrix multiplication. 
Graph matrices are much sparser and have more random distribution 
than traditional sparse matrices that arise in scientific
computing. As such, many optimizations for traditional sparse matrices, such
as register blocking and prefetching \cite{Williams07} increase storage size,
bring more data to CPU cache unnecessarily and hurt performance.
The commonly used sparse matrix storage formats such as CSR (compressed sparse
row) and CSC (compressed sparse column) for graph matrices leads to significant
CPU cache misses
in sparse matrix multiplication due to near-random distribution of non-zero
entries as well as the sparsity and the enormous size of graphs. 

We focus on sparse matrices derived from graphs because they are more difficult 
to compute efficiently than general sparse matrices and because some of the 
largest matrices and most important analysis tasks operate on graph datasets, 
e.g.~social networks and Web graphs. 
Facebook's social network has billions of vertices and
the Web Data Commons graph \cite{web_graph} has 3.6 billion vertices and 128
billions edges. We perform sparse matrix multiplication for graph
analysis such as community detection with NMF and spectral analysis with SVD.

%These matrices inherit structure
%from natural graphs. 
%
%They are typically very sparse and have near-random
%distribution for non-zero entries. They also have a power law distribution
%that governs the number of non-zero entries per row and column.



%Furthermore, graphs cannot be
%clustered or partitioned effectively \cite{leskovec} to localize access.
%Therefore, sparse matrix multiplication on graphs is frequently the bottleneck
%in an application.

%How have others addressed the problem?
Current research focuses on sparse matrix multiplication in memory
for small matrices and scaling to a large sparse matrix in a large cluster,
where the aggregate memory is sufficient to store the sparse matrix
\cite{Williams07, Yoo11, Boman2013}.
Distributed solutions for sparse matrix multiplication lead to significant
network communication and network bandwidth is usually the bottleneck.
As such, this operation requires a fast network to achieve performance.
A supercomputer or a large cluster with a fast network is inaccessible or
too expensive for many users. In addition, the distributed solution imposes
challenges in achieving load balancing on power-law graphs.

%What is the nature of your solution?
On the other hand, a current trend for hardware design scales up
a single machine for high performance computing, rather than scaling out to 
many networked machines.
These machines typically have multiple processors with many CPU cores and
a large amount of memory. They are also equipped with fast flash
memory such as solid-state drives (SSDs) to further extend memory capacity.
This conforms to the node design for supercomputers \cite{Ang14}.
Recent finding, from Frank McSheery \cite{McSherry15,McSherryBlog} and our
prior work \cite{FlashGraph},
show that the largest graph analytics tasks can be done on a small fraction of
the hardware, at less cost, as fast, and using less energy on a single shared-memory 
node, rather than a distributed compute engine.  Our finding in this paper reveal
that sparse matrices have the same structure and benefits and that the largest 
matrices can be processed efficiently on scale-up compute nodes.

We explore a solution that scales sparse matrix dense matrix multiplication
(SpMM) on a multi-core machine with commodity SSDs and
perform SpMM in semi-external memory (SEM). The concept of semi-external memory
arose as a functional computing approach for graphs \cite{Abello98} in which
the vertex state of a graph is stored in memory and the edges accessed from
external memory. We introduce a similar construct for SpMM in which one or more
columns of a dense matrix are kept in memory and the sparse matrix is accessed
from external memory. In semi-external memory, we assume
that the memory of a machine is sufficient to keep at least one column
of the input dense matrix but is insufficient to hold the sparse matrix
and the dense matrices. Given fast SSDs, we demonstrate that the SEM
solution uses the resources of a multi-core machine well and
achieves performance that exceeds the state-of-the-art in-memory implementations.
%while increasing the scalability in proportion
%to the ratio of non-zero entries to rows or columns in a sparse matrix.

% Why is it new/different/special?

We overcome many technical challenges to construct a sparse matrix
multiplication implementation on SSDs to achieve performance. Specifically,
SSDs are an order of magnitude slower in throughput and multiple orders of
magnitude slower in latency than DRAM. Furthermore, sequential I/O of SSDs
is still much faster
than random I/O \cite{safs} and reads are faster than writes. In addition,
SSDs wear out when we write data to them and random writes further shorten
the lives of SSDs \cite{sfs}. As such, our solution needs to sequentialize
I/O access and reduce I/O, especially writes.

Semi-external memory provides a scalable and efficient SpMM solution that
meets the I/O challenges and incorporates substantial computation optimizations
to achieve performance of SpMM on graphs in a NUMA (non-uniform memory
architecture) machine. During the computation, each
thread streams its own partitions of the sparse matrix from SSDs, maximizing
I/O throughput and avoiding thread synchronization. We buffer all
intermediate computation results in local memory, to reduce remote memory
access, and stream the output matrix to SSDs at most once, minimizing writes
to SSDs. We design a very compact sparse matrix format to accelerate retrieving
a sparse matrix from SSDs. Semi-external memory has memory constraints.
As such, we deploy only computation optimizations that require a small memory
footprint, e.g., fine-grain dynamic task scheduling to balance loads on
power-law graphs and cache blocking to increase CPU cache hits.

Given the sparsity and the enormous size of graphs, the dense matrices involved
in SpMM are massive and our semi-external memory solution adapts to
dense matrices with different numbers of columns. When the dense matrix is
larger than memory, we split it
vertically into multiple partitions so that each partition can fit in
memory. As such, the minimum memory requirement of our solution is $O(r)$,
where $r$ is the number of rows in the input dense matrix. By keeping more columns
in the dense matrix in memory, we reduce I/O from SSDs and SEM-SpMM becomes
CPU bound, instead of I/O bound, on fast SSDs.

We develop three important applications in scientific computing and graph
analysis
with our SEM-SpMM: PageRank \cite{pagerank}, eigensolver \cite{anasazi} and
non-negative matrix factorization \cite{nmf}. Each of them requires SpMM with
different numbers of columns in dense matrices, resulting in different
strategies of placing data in memory. With the three applications, we
demonstrate data placement choices for different memory capacities in a machine
and the impact of the memory size on the performance of the applications.

% What are it's key features?

The main contributions of this paper are:
\begin{itemize}
	\item We scale SpMM in semi-external memory with SSDs and tailor runtime
		optimizations that meet memory constraints in the semi-external memory
		setting and asymmetric I/O access of SSDs.
	\item We deploy a compact sparse matrix format to reduce the amount of
		I/O and alleviate I/O as the bottleneck of the system.
	\item We deploy a fine-grain dynamic load balancing scheme for sparse
		matrices with power-law distribution in non-zero entries.
	\item We optimize SpMM for dense matrices with various numbers of columns
		and extends SEM-SpMM to large dense matrices that cannot fit in memory
		by vertically partitioning the dense matrices.
%	\item We demonstrate that SpMM in semi-external memory, coupled with SSDs,
%		meets or even exceeds the performance of highly optimized in-memory
%		implementations. Our SEM-SpMM achieves almost 100\% performance of our
%		in-memory implementation on a large parallel machine. Even for SpMV,
%		our SEM implementation achieves at least 65\% performance of our
%		in-memory implementation and outperforms Trilinos \cite{trilinos} and
%		MKL \cite{mkl} by a factor of $2-9$.
%	\item The applications implemented with our SpMM significantly outperform
%		the state-of-the-art implementations of these applications.
%	\item The code of this work is released as open source at http://flashx.io.
\end{itemize}

Our result shows that for real-world sparse graphs, our SEM-SpMM achieves almost
100\% performance of our in-memory implementation on a large parallel machine
with 48 CPU cores when the dense matrix has more than four columns. Even for
SpMV, our SEM implementation achieves at least 65\% performance of our in-memory
implementation and outperforms Trilinos \cite{trilinos} and MKL \cite{mkl} by
a factor of $2-9$. The applications implemented with our SpMM significantly
outperform the state-of-the-art implementations of these applications. As such,
we conclude that semi-external memory coupled with SSDs delivers an efficient
solution for large-scale sparse matrix multiplication. It serves
as a building block and offers new design possibilities for large-scale
data analysis, replacing memory with larger, cheaper, more energy-efficient SSDs
and processing bigger problems on fewer machines. The code for SpMM and SpMV has been 
released as open source as part of the FlashX system at http://flashx.io.
