% What problem are you going to solve.
Sparse matrix multiplication is very important computation with a wide variety
of applications in scientific computing, machine learning and data mining.
For example, many matrix factorization algorithms on a sparse matrix such as
singular value decomposition (SVD) \cite{svd} and non-negative matrix
factorization (NMF) \cite{nmf} requires sparse matrix multiplication.
Many graph analysis algorithms such as PageRank \cite{pagerank} can be
formulated as sparse matrix multiplication \cite{Mattson13}. Some of
the algorithms such as PageRank and SVD require sparse matrix vector
multiplication while some others such as NMF require sparse matrix dense
matrix multiplication.

% Why is it hard?

It is challenging to implement an efficient kernel of sparse matrix
multiplication, especially for many real-world graphs such as social networks
and Web graphs. Sparse matrix multiplication is notorious for achieving only
a small fraction of the peak performance of a modern processor \cite{Williams07}.
It becomes even more challenging to perform this operation on graphs due to
random memory access caused by near-random vertex connection and load imbalancing
caused by the power-law distribution in vertex degree. Many real-world graphs
are enormous. For example, Facebook's social graph has billions of vertices
and today's web graphs are much large. Furthermore, graphs cannot be
clustered or partitioned effectively \cite{leskovec} to localize access.
Therefore, sparse matrix multiplication on graphs is frequently the bottleneck
in an application.

%How have others addressed the problem?
Current research focuses on sparse matrix vector multiplication in memory for
small graphs and process large graphs in a large
cluster, where the aggregate memory is sufficient to store the graphs.
The distributed solution for sparse matrix multiplication leads to significant
network communication and is usually bottlenecked by the network.
As such, this operation requires fast network to achieve performance.
However, a supercomputer or a large cluster with fast network communication
is not accessible to many people.

\dz{can we demonstrate the distributed solution indeed has scalability problems?}

%What is the nature of your solution?

We explore an alternative solution that scales sparse matrix multiplication
by performing this operation in a semi-external memory (SEM) fashion on commodity
solid-state drives (SSDs). Today's SSDs or SSD arrays are capable of delivering
over a million random IOPS or multiple GB/s of sequential I/O \cite{safs}.
Given such I/O performance, we demonstrate that the SEM solution can achieve
performance comparable to
state-of-art in-memory implementation while increasing the scalability
in proportion to the ratio of edges to vertices in a graph. Semi-external
memory \cite{flashgraph, Abello98} keeps the sparse matrix on SSDs and
the vector or the dense matrix in memory.

%Given such I/O performance, it is possible to
%achieve nearly in-memory performance for large-scale graph analysis using SSDs
%\cite{flashgraph}. In this paper, we focus on utilizing SSDs for
%spectral graph analysis as well as general linear algebra.

% Why is it new/different/special?

Although SSDs can deliver high IOPS and sequential I/O throughput, we have
to overcome many technical challenges to construct an SSD-friendly kernal
of sparse matrix multiplication to achieve performance comparable to in-memory
counterparts. Even though SSDs have high IOPS, their sequential I/O throughput
is still significantly higher than random I/O. Furthermore, random I/O
consumes substantial CPU computation \cite{safs}. Even if we achieve
the maximal I/O throughput, SSDs are still an order of magnitude slower
than DRAM in throughput. Furthermore, random write is harmful to SSDs
\cite{sfs}. It increases write amplification, decreases I/O throughput
and shortens the lives of SSDs.

Semi-external memory provides a scalable sparse matrix multiplication solution
that is friendly for SSDs, parallelization and in-memory optimizations.
It streams the sparse matrix from SSDs for computation and results in maximal
I/O throughput from SSDs. While maximizing I/O throughput, we also compress
the sparse matrix to further accelerate retrieving the sparse matrix from SSDs.
In the parallel setting, each thread streams its own partitions for computation
and streams its portions of computation result to SSDs. As such, the computation
in each thread is independent to each other.
We deploy multiple in-memory optimizations specifically designed for power-law
graphs. For example, we assign partitions of the sparse matrix dynamically to
threads for load balancing, deploy cache blocking to increase CPU cache hits,
partition and evenly distribute the dense matrix to NUMA nodes to fully utilize
the memory bandwidth of a NUMA machine.

Sparse matrix dense matrix multiplication has much higher computation density
than sparse matrix vector multiplication.
When performing sparse matrix dense matrix multiplication, we organize
the dense matrix in the row-major order to increase data locality.
When the dense matrix is larger than the memory, we split the dense matrix
vertically into multiple partitions. Each partition can fit in memory and
is organized in row-major order.
This approach requires $\lceil \frac{D}{M} \rceil$ passes over the sparse
matrix. When RAM becomes the bottleneck, it does not really matter.

Semi-external memory has a subtle point. The bottleneck in the system may
vary as the number of columns in the dense matrix increases.

% What are it's key features?

energy-efficient.

The semi-external memory sparse matrix multiplication is usually bottlenecked
by RAM instead of SSDs. Discuss how much I/O is required to balance SSDs and
RAM.

Our result shows that for many real-world sparse graphs, the SSD-based
eigensolver is able to achieve 40\%-60\%
performance of its in-memory implementation and has performance comparable to
the Anasazi eigensolver on a machine with 48 CPU cores for computing various
numbers of eigenvalues. We further demonstrate that the SSD-based eigensolver
is capable of scaling to a graph with 3.4 billion vertices and 129 billion edges.
It takes about 4 hours to compute eight eigenvalues of the billion-node graph
and use 120 GB memory. We conclude that our solution offers new design
possibilities for large-scale eigendecomposition, replacing memory with larger
and cheaper SSDs and processing bigger problems on fewer machines.
