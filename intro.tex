% What problem are you going to solve.
Sparse matrix multiplication is a very important computation with a wide variety
of applications in scientific computing, machine learning and data mining.
For example, matrix factorization algorithms on a sparse matrix such as
singular value decomposition (SVD) \cite{svd} and non-negative matrix
factorization (NMF) \cite{nmf} requires sparse matrix multiplication.
Graph analysis algorithms such as PageRank \cite{pagerank} can be
formulated as sparse matrix multiplication or generalized sparse matrix
multiplication \cite{Mattson13}. Some of
the algorithms, such as PageRank and SVD, require sparse matrix vector
multiplication. Others, such as NMF, require sparse matrix dense
matrix multiplication.

The largest sparse matrices arise from graph datasets such as social networks
and Web graphs, in which one performs
sparse matrix multiplication for graph analysis such as community detection
with NMF and spectral analysis with SVD. These matrices inherit structure
from natural graphs. Specifically,
these matrices are typically very sparse and have near-random distribution
for non-zero entries. They also have a power law distribution that governs
the number of non-zero entries per row and column.

% Why is it hard?

It is challenging to have an efficient implementation of sparse matrix
multiplication, especially for sparse matrices that encode real-world graphs.
Sparse matrix multiplication is notorious for achieving only
a small fraction of the peak performance of a modern processor \cite{Williams07}.
It becomes even more challenging to perform this operation on graphs due to
random memory access caused by near-random vertex connection and load imbalancing
caused by the power-law distribution in vertex degree. Furthermore, many
real-world graphs
are enormous. For example, Facebook's social network has billions of vertices
and today's Web graphs are even much larger. %Furthermore, graphs cannot be
%clustered or partitioned effectively \cite{leskovec} to localize access.
Therefore, sparse matrix multiplication on graphs is frequently the bottleneck
in an application.

%How have others addressed the problem?
Current research focuses on sparse matrix vector multiplication (SpMV) in memory
for small matrices and scaling to a large sparse matrix in a large cluster,
where the aggregate memory is sufficient to store the sparse matrix
\cite{Williams07, Yoo11, Boman2013}.
The distributed solution for sparse matrix multiplication leads to significant
network communication and network bandwidth is usually the bottleneck.
As such, this operation requires a fast network to achieve performance.
A supercomputer or a large cluster with a fast network is inaccessible or
too expensive for many users.

%What is the nature of your solution?

On the other hand, a current trend for hardware design is to scale up
a single machine for high performance computing.
These machines typically have multiple processors with many CPU cores and
a large amount of memory. They are also equipped with fast flash
memory such as solid-state drives (SSDs) to further extend memory capacity.
This conforms to the node design for supercomputers \cite{Ang14}.

We explore a solution that scales sparse matrix dense matrix multiplication
(SpMM) on a multi-core machine with commodity SSDs and
perform SpMM in semi-external memory (SEM). The concept of semi-external memory
arose as a functional computing approach for graphs \cite{Abello98} in which
the vertex state of a graph is stored in memory and the edges accessed from
external memory. We introduce a similar construct for SpMM in which one or more
columns of a dense matrix are kept in memory and the sparse matrix is accessed
from external memory. In semi-external memory, we assume
that the memory of a machine is sufficient to keep at least one column
of the input dense matrix but is insufficient to hold the sparse matrix
and the dense matrices in memory. Even though SpMM could be implemented with
SpMV, such an implementation would fail to explore data locality in SpMM and
result in higher I/O access in semi-external memory. We optimize SpMM directly
to overcome these problems. Given fast SSDs, we demonstrate that the SEM
solution uses the resources of a multi-core machine well and
achieves performance comparable to state-of-the-art in-memory implementations
for sparse matrix multiplication while increasing the scalability in proportion
to the ratio of non-zero entries to rows or columns in a sparse matrix.

% Why is it new/different/special?

We have to overcome many technical challenges to construct a sparse matrix
multiplication implementation on SSDs to achieve performance. Even though SSDs
are fast, they are still an order of magnitude slower than DRAM in throughput
and multiple orders of magnitude slower than DRAM in latency.
To maximize I/O throughput, sequential I/O is preferred because it has much
higher throughput than random I/O \cite{safs}. In addition, random writes are
harmful to SSDs \cite{sfs} because
they decrease I/O throughput and shorten the lives of SSDs.

Semi-external memory provides a scalable SpMM solution that incorporates well
with I/O access to SSDs, parallelization and in-memory optimizations.
It streams the sparse matrix from SSDs for computation, which results in maximal
I/O throughput from SSDs. It streams the output matrix to SSDs only once if
the memory is insufficient to store the output matrix, minimizing writes to
SSDs and maximizing I/O throughput.
We compress the sparse matrix to further accelerate retrieving the sparse
matrix from SSDs. In the parallel setting, each thread streams its own partitions
of the sparse matrix to memory and performs computation.
We deploy multiple in-memory optimizations specifically designed for power-law
graphs. For example, we assign partitions of the sparse matrix dynamically to
threads for load balancing, deploy cache blocking to increase CPU cache hits,
distribute the dense matrix to NUMA nodes to fully utilize the memory
bandwidth of a NUMA machine, and organize the dense matrix in the row-major order
to explore data locality in SpMM.

Our semi-external memory solution adapts to machines with different memory
capacities. When the dense matrix is larger than memory, we split the dense
matrix vertically into multiple partitions so that each partition can fit in
memory. As such, the minimum memory requirement of our solution is $O(n)$,
where $n$ is the number of rows in the dense matrix. By keeping more columns
in the dense matrix in memory, we reduce I/O from SSDs in SpMM. When the number
of columns in a dense matrix increases, SEM SpMM becomes CPU bound, instead of
I/O bound on fast SSDs.

We develop three important applications in scientific computing and data mining
with our SEM SpMM: PageRank \cite{pagerank}, eigensolver \cite{anasazi} and
non-negative matrix factorization \cite{nmf}. Each of them requires SpMM with
different numbers of columns in dense matrices, resulting in different
strategies of placing data in memory. With the three applications, we
demonstrate data placement choices for different memory capacities in a machine
and the impact of the memory size on the performance of the applications.

% What are it's key features?

Our result shows that for real-world sparse graphs, our SEM sparse matrix
multiplication achieves almost 100\% performance of our in-memory implementation
on a large parallel machine with 48 CPU cores
when the dense matrix has more than four columns. Even for sparse matrix vector
multiplication, our SEM implementation achieves at least 65\% performance of
our in-memory implementation and significantly outperforms Trilinos \cite{trilinos}
and MKL \cite{mkl}. The applications implemented with our SpMM significantly
outperform the state-of-the-art implementations of these applications. As such,
we conclude that semi-external memory coupled with SSDs delivers an efficient
solution for large-scale sparse matrix multiplication. It also serves
as a building block and offers new design possibilities for large-scale
data analysis, replacing memory with larger, cheaper, more energy-efficient SSDs
and processing bigger problems on fewer machines. The code of this work is
released as open source at http://flashx.io.
