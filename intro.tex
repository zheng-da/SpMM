% What problem are you going to solve.
Sparse matrix multiplication is very important computation with a wide variety
of applications in scientific computing, machine learning and data mining.
For example, many matrix factorization algorithms on a sparse matrix such as
singular value decomposition (SVD) \cite{svd} and non-negative matrix
factorization (NMF) \cite{nmf} requires sparse matrix multiplication.
Many graph analysis algorithms such as PageRank \cite{pagerank} can be
formulated as sparse matrix multiplication \cite{Mattson13}. Some of
the algorithms such as PageRank and SVD require sparse matrix vector
multiplication while some others such as NMF require sparse matrix dense
matrix multiplication.

% Why is it hard?

It is challenging to implement an efficient kernel of sparse matrix
multiplication, especially for many real-world graphs such as social networks
and Web graphs. Sparse matrix multiplication is notorious for achieving only
a small fraction of the peak performance of a modern processor \cite{Williams07}.
It becomes even more challenging to perform this operation on graphs due to
random memory access caused by near-random vertex connection and load imbalancing
caused by the power-law distribution in vertex degree. Many real-world graphs
are enormous. For example, Facebook's social graph has billions of vertices
and today's web graphs are much large. Furthermore, graphs cannot be
clustered or partitioned effectively \cite{leskovec} to localize access.
Therefore, sparse matrix multiplication on graphs is frequently the bottleneck
in an application.

%How have others addressed the problem?
Current research focuses on sparse matrix vector multiplication in memory for
small matrices and processing a large sparse matrix requires a large cluster,
where the aggregate memory is sufficient to store the sparse matrix \cite{}.
The distributed solution for sparse matrix multiplication leads to significant
network communication and is usually bottlenecked by the network.
As such, this operation requires fast network to achieve performance.
However, a supercomputer or a large cluster with fast network communication
is inaccessible or too expensive to many people.

\dz{can we demonstrate the distributed solution indeed has scalability problems?}

%What is the nature of your solution?

We explore an alternative solution that scales sparse matrix multiplication
by utilizing commodity solid-state drives (SSDs) and perform this operation
in a semi-external memory (SEM) fashion. Today's SSDs or SSD arrays are capable
of delivering over a million random IOPS or multiple GB/s of sequential I/O
\cite{safs}. Given such I/O performance, we demonstrate that the SEM solution
can achieve performance comparable to state-of-art in-memory implementations
for sparse matrix multiplication while increasing the scalability in proportion
to the ratio of non-zero entries to rows or columns in a sparse matrix.
Semi-external memory \cite{flashgraph, Abello98} keeps the sparse matrix on SSDs
and only keep the vector or the dense matrix in memory.

% Why is it new/different/special?

Although SSDs can deliver high IOPS and sequential I/O throughput, we have
to overcome many technical challenges to construct an SSD-friendly kernal
of sparse matrix multiplication to achieve performance comparable to in-memory
counterparts. Even though SSDs have high IOPS, their sequential I/O throughput
is still significantly higher than random I/O. Even if we achieve
the maximal I/O throughput, SSDs are still an order of magnitude slower
than DRAM in throughput. Furthermore, random write is harmful to SSDs
\cite{sfs}. It increases write amplification, decreases I/O throughput
and shortens the lives of SSDs.

Semi-external memory provides a scalable sparse matrix multiplication solution
that is friendly for SSDs, parallelization and in-memory optimizations.
It streams the sparse matrix from SSDs for computation, which results in maximal
I/O throughput from SSDs. It streams the output vector or matrix to SSDs if
memory is insufficient to store the output vector or matrix, which results in
the minimum amount of data written to SSDs with large I/O. While maximizing I/O
throughput, we also compress
the sparse matrix to further accelerate retrieving the sparse matrix from SSDs.
In the parallel setting, each thread streams its own partitions for computation
and streams its portions of computation result to SSDs. As such, the computation
in each thread is independent to each other.
We deploy multiple in-memory optimizations specifically designed for power-law
graphs. For example, we assign partitions of the sparse matrix dynamically to
threads for load balancing, deploy cache blocking to increase CPU cache hits,
partition and evenly distribute the dense matrix to NUMA nodes to fully utilize
the memory bandwidth of a NUMA machine.

Sparse matrix dense matrix multiplication has much higher computation density
than sparse matrix vector multiplication. When the number of columns in
a dense matrix increases, the system becomes CPU or memory-bound, instead of
I/O bound.
When performing sparse matrix dense matrix multiplication, we organize
the dense matrix in the row-major order to increase data locality.
When the dense matrix is larger than the memory, we split the dense matrix
vertically into multiple partitions. Each partition can fit in memory and
is organized in row-major order.

% What are it's key features?

energy-efficient.

The semi-external memory sparse matrix multiplication is usually bottlenecked
by RAM instead of SSDs. Discuss how much I/O is required to balance SSDs and
RAM.

Our result shows that for many real-world sparse graphs, the SSD-based
eigensolver is able to achieve 40\%-60\%
performance of its in-memory implementation and has performance comparable to
the Anasazi eigensolver on a machine with 48 CPU cores for computing various
numbers of eigenvalues. We further demonstrate that the SSD-based eigensolver
is capable of scaling to a graph with 3.4 billion vertices and 129 billion edges.
It takes about 4 hours to compute eight eigenvalues of the billion-node graph
and use 120 GB memory. We conclude that our solution offers new design
possibilities for large-scale eigendecomposition, replacing memory with larger
and cheaper SSDs and processing bigger problems on fewer machines.
