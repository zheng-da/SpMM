% What problem are you going to solve.
Sparse matrix multiplication is very important computation with a wide variety
of applications in scientific computing, machine learning and data mining.
For example, many matrix factorization algorithms on a sparse matrix such as
singular value decomposition (SVD) \cite{svd} and non-negative matrix
factorization (NMF) \cite{nmf} requires sparse matrix multiplication.
Many graph analysis algorithms such as PageRank \cite{pagerank} can be
formulated as sparse matrix multiplication \cite{Mattson13}. Some of
the algorithms such as PageRank and SVD require sparse matrix vector
multiplication while some others such as NMF require sparse matrix dense
matrix multiplication.

% Why is it hard?

It is challenging to implement an efficient kernel of sparse matrix
multiplication, especially for many real-world graphs such as social networks
and Web graphs. Sparse matrix multiplication is notorious for achieving only
a small fraction of the peak performance of a modern processor \cite{Williams07}.
It becomes even more challenging to perform this operation on graphs due to
random memory access caused by near-random vertex connection and load imbalancing
caused by the power-law distribution in vertex degree. Many real-world graphs
are enormous. For example, Facebook's social graph has billions of vertices
and today's web graphs are much large. Furthermore, graphs cannot be
clustered or partitioned effectively \cite{leskovec} to localize access.
Therefore, sparse matrix multiplication on graphs is frequently the bottleneck
in an application.

%How have others addressed the problem?
Current research focuses on sparse matrix vector multiplication in memory for
small matrices and processing a large sparse matrix requires a large cluster,
where the aggregate memory is sufficient to store the sparse matrix \cite{}.
The distributed solution for sparse matrix multiplication leads to significant
network communication and is usually bottlenecked by the network.
As such, this operation requires fast network to achieve performance.
However, a supercomputer or a large cluster with fast network communication
is inaccessible or too expensive to many people.

\dz{can we demonstrate the distributed solution indeed has scalability problems?}

%What is the nature of your solution?

We explore an alternative solution that scales sparse matrix multiplication
by utilizing commodity solid-state drives (SSDs) and perform this operation
in a semi-external memory (SEM) fashion \cite{flashgraph, Abello98}.
That is, we keep the sparse matrix on SSDs and only keep the vector or
the dense matrix in memory. Today's SSDs or SSD arrays are capable of delivering
over a million random IOPS or multiple GB/s of sequential I/O \cite{safs}.
Furthermore, SSDs are energy-efficient storage media \cite{}. Given fast SSDs,
we demonstrate that the SEM solution
can achieve performance comparable to state-of-art in-memory implementations
for sparse matrix multiplication while increasing the scalability in proportion
to the ratio of non-zero entries to rows or columns in a sparse matrix.

% Why is it new/different/special?

Although SSDs can deliver high IOPS and sequential I/O throughput, we have
to overcome many technical challenges to construct an SSD-friendly kernal
of sparse matrix multiplication to achieve performance comparable to in-memory
counterparts. Even though SSDs have high IOPS, their sequential I/O throughput
is still significantly higher than random I/O. Even if we achieve
the maximal I/O throughput, SSDs are still an order of magnitude slower
than DRAM in throughput. Furthermore, random write is harmful to SSDs
\cite{sfs}. It increases write amplification, decreases I/O throughput
and shortens the lives of SSDs.

Semi-external memory provides a scalable sparse matrix multiplication solution
that is friendly for SSDs, parallelization and in-memory optimizations.
It streams the sparse matrix from SSDs for computation, which results in maximal
I/O throughput from SSDs. It streams the output vector or matrix to SSDs if
memory is insufficient to store the output vector or matrix, which results in
the minimum amount of data written to SSDs with large I/O. While maximizing I/O
throughput, we also compress
the sparse matrix to further accelerate retrieving the sparse matrix from SSDs.
In the parallel setting, each thread streams its own partitions for computation
and streams its portions of computation result to SSDs. As such, the computation
in each thread is independent to each other.
We deploy multiple in-memory optimizations specifically designed for power-law
graphs. For example, we assign partitions of the sparse matrix dynamically to
threads for load balancing, deploy cache blocking to increase CPU cache hits,
partition and evenly distribute the dense matrix to NUMA nodes to fully utilize
the memory bandwidth of a NUMA machine.

Sparse matrix dense matrix multiplication has much higher computation density
than sparse matrix vector multiplication. When the number of columns in
a dense matrix increases, the system becomes CPU or memory-bound, instead of
I/O bound.
When performing sparse matrix dense matrix multiplication, we organize
the dense matrix in the row-major order to increase data locality.
When the dense matrix is larger than the memory, we split the dense matrix
vertically into multiple partitions. Each partition can fit in memory and
is organized in row-major order.

% What are it's key features?

Our result shows that for many real-world sparse graphs, our SEM sparse matrix
multiplication can achieve at least 60\% performance of its in-memory
implementations and significantly outperforms Trilinos \cite{trilinos} and
MKL \cite{mkl} on a machine with 48 CPU cores. As the number of columns in
the dense matrix increases, the performance gap between the in-memory and
SEM implementations shrinks. When the dense matrix has four or eight columns,
the system becomes CPU bottlenecked and the SEM solution achieves almost
100\% performance of the in-memory implementation for some of our graphs.
This suggests that the SEM solution requires sufficient memory to achieve
performance. However, additional memory cannot futher improve performance
but waste energy. We apply the SEM sparse matrix multiplication to some important
data analysis applications that require sparse matrix multiplication and compete
our solution with the state-of-art implementations of these applications.
\dz{show some brief performance results.} As such, we conclude that
semi-external memory coupled with SSDs delivers a fast and energy-efficient
solution for large-scale sparse matrix multiplication. It can also serves
as a building block and offers new design possibilities for large-scale
data analysis, replacing memory with larger, cheaper, more energy-efficient SSDs
and processing bigger problems on fewer machines.
