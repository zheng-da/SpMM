Recent sparse matrix multiplication studies focus on in-memory optimizations
for sparse matrix vector multiplication (SpMV). Williams et al.~\cite{Williams07}
describe optimizations for SpMV in multicore architecture. Yoo et al.~\cite{Yoo11}
and Boman et al.~\cite{Boman2013} optimize distributed SpMV for large
scale-free graphs with 2D edge partitioning to reduce communication between
machines. In contrast, Sparse matrix dense matrix multiplication (SpMM) receives
less attention from the high-performance computing community. Even though
SpMM can be implemented with SpMV, SpMV fails to explore data locality in SpMM.
Aktulga et al.~\cite{Aktulga14} optimizes SpMM with cache blocking. We advance
SpMM with a focus on optimizations for semi-external memory.

Compressed row storage (CSR) or compressed column storage (CSC) format are commonly
used sparse matrix formats in many numeric libraries such as Intel MKL \cite{mkl}
and Trilinos \cite{trilinos}. However, these formats are not designed for graphs
and sparse matrix multiplication with these formats on graphs incurs many random
memory accesses.
More modern sparse matrix formats have been designed.
Sparsity \cite{Im04} proposes both register blocking and cache blocking to
increase data reuse in CPU cache for sparse matrix multiplication. Register blocking
requires to explicitly store zero values in register blocks. This strategy
potentially wastes space and computation for graphs because graphs are very
sparse and have nearly random vertex connection. Buluc et al.~\cite{Buluc08}
further advances sparse matrix format
by doubly compressed sparse column (DCSC) for hypersparse submatrices after 2D
partitioning on a sparse matrix. This format significantly reduces the storage
size of a 2D-partitioned sparse matrix. We adopt some of the optimizations
and further advance the sparse matrix
format with a focus on reducing the storage size of a sparse matrix.

Abello et al.~\cite{Abello98} introduced the semi-external memory algorithmic
framework for graphs. Pearce et al.~\cite{Pearce10} implement several 
semi-external memory graph traversal algorithms for SSDs. FlashGraph
\cite{flashgraph} adopted the concept and performs graph algorithms with
vertex state in memory and edge lists on SSDs. This work extends the semi-external
memory concept to matrix operations.

Zhou et al.~\cite{Zhou12} implemented an LOBPCG \cite{Arbenz05} eigensolver in
an SSD cluster. Their implementation targets nuclear many-body Hamiltonian
matrices, which are much denser and have smaller dimensions than many sparse
graphs. Therefore, their solution stores the sparse matrix on SSDs and keep
the entire vector subspace in RAM. They focus on optimizations
in the distributed environment. In contrast, our eigensolver based on our
SEM SpMM stores both
the sparse matrix and the vector subspace on SSDs due to the large number
of vertices in our target graphs. We focus on external-memory optimizations
in a single machine.

Anasazi \cite{anasazi} is an eigensolver framework in the Trilinos project
\cite{trilinos}. This framework implements block extension of multiple
eigensolver algorithms such as Block Krylov-Schur \cite{krylovschur},
Block Davidson \cite{Arbenz05} and LOBPCG \cite{Arbenz05}.
This is a very flexible framework
that allows users to redefine sparse matrix dense matrix multiplication and
dense matrix operations. By default, Anasazi uses the matrix implementations
in Trilinos that runs in distributed memory.

Intel Math Kernel Library \cite{mkl} is an efficient and parallel linear
algebra library with matrix operations specifically optimized for Intel
platforms. It provides an efficient sparse matrix multiplication optimized
for regular sparse matrices. In contrast, our sparse matrix multiplication
optimizes for power-law graphs with near-random vertex connection.
