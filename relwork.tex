Recent sparse matrix multiplication studies focus on in-memory optimizations.
Williams et al.~\cite{Williams07} describe optimizations for sparse matrix
vector multiplication (SpMV) in multicore architectures. Yoo et al.~\cite{Yoo11}
and Boman et al.~\cite{Boman2013} optimize distributed SpMV for large
scale-free graphs with 2D partitioning to reduce communication between
machines. In contrast, Sparse matrix dense matrix multiplication (SpMM) receives
less attention from the high-performance computing community. Even though
SpMM can be implemented with SpMV, SpMV fails to explore data locality in
SpMM. Aktulga et al.~\cite{Aktulga14} optimize SpMM with cache blocking.
Koanantakool et al.~\cite{Koanantakool16} experiment different parallel
algorithms for sparse matrix dense matrix multiplication and analyze
their communication cost in distributed memory.
We advance SpMM with a focus on optimizations for semi-external memory.

Compressed row storage (CSR) and compressed column storage (CSC) formats are commonly
used sparse matrix formats in many numeric libraries such as Intel MKL \cite{mkl}
and Trilinos \cite{trilinos}. However, these formats are not designed for graphs.
Sparse matrix multiplication with these formats on graphs incurs many random memory
accesses. %Other, more modern sparse matrix formats have been designed.
Sparsity \cite{Im04} designs a format that encodes both register blocking and cache blocking to
increase data reuse in the CPU cache for sparse matrix multiplication. Register blocking
requires explicit storage of zero values in register blocks. This strategy
wastes space and computation for graphs because graphs are very
sparse and have nearly random vertex connection. Buluc et al.~\cite{Buluc08}
further advance sparse matrix format
by doubly compressed sparse column (DCSC) for hypersparse submatrices after 2D
partitioning on a sparse matrix. This format significantly reduces the storage
size of a 2D-partitioned sparse matrix. Our format further reduces the storage
size of a sparse matrix. SciDB \cite{Stonebraker11} developed a general purpose
dense array format that includes 2-d and 3-d partitioning and ghost cells
\cite{Soroush11}.  It is not well-suited to sparse matrixes.

%DZ TODO scidb cite -- http://www.odbms.org/wp-content/uploads/2014/04/The_Architecture_of_SciDB.pdf
% Array Store cite -- http://scidb.cs.washington.edu/paper/sigmod362-soroush.pdf

Abello et al.~\cite{Abello98} introduced the semi-external memory algorithmic
framework for graphs. Pearce et al.~\cite{Pearce10} implement several 
semi-external memory graph traversal algorithms for SSDs. FlashGraph
\cite{FlashGraph} adopted the concept and performs graph algorithms with
vertex state in memory and edge lists on SSDs. This work extends the semi-external
memory concept to matrix operations.

GridGraph \cite{gridgraph} is a general-purpose graph processing framework on
a single machine
and scales to large graphs using disks. It performs 2D partitioning on a graph
to reduce CPU cache misses in graph analysis and deploys 2-level hierarchical
partitioning to reduce I/O. This graph engine is designed for graph
algorithms expressed as sparse matrix vector multiplication and keeps both
graphs and vertex computation state on disks. In contrast, our work focuses on
optimizations on sparse matrix dense matrix multiplication and performs this
operation in semi-external memory to reduce I/O, especially to minimize writes
to SSDs. GridGraph runs much more slowly than our SEM SpMM implementation.
%\dz{Do we need to compare with it directly?}

MapReduce \cite{MapReduce} is also commonly used for processing large graphs.
PEGASUS \cite{pegasus} is a popular graph processing engine built on MapReduce
and expresses graph algorithms as a generalized form of sparse matrix-vector
multiplication. GBase \cite{gbase} is a graph database built on MapReduce.
It optimizes the graph storage for graph queries and analysis and perform graph
queries and analysis with sparse matrix vector multiplication.
Other work \cite{Liao14, Yin14, Liu10} implement non-negative matrix
factorization (NMF) with MapReduce. Although these MapReduce-based
implementations can scale to very large graphs, they run orders of
magnitude more slowly than optimized solutions, such as Trillinos and Intel MKL.

Zhou et al.~\cite{Zhou12} implement an LOBPCG \cite{Arbenz05} eigensolver in
an SSD cluster. Their implementation targets nuclear many-body Hamiltonian
matrices, which are much denser and have smaller dimensions than many sparse
graphs. Therefore, their solution stores the sparse matrix on SSDs and keep
the entire vector subspace in RAM. In contrast, our SEM-SpMM handles dense
matrices of different sizes and our eigensolver stores both the sparse matrix
and the vector subspace on SSDs.

Anasazi \cite{anasazi} is an eigensolver framework in the Trilinos project
\cite{trilinos}. This framework implements block extension of multiple
eigensolver algorithms such as Block Krylov-Schur \cite{krylovschur},
Block Davidson \cite{Arbenz05} and LOBPCG \cite{Arbenz05}. Anasazi uses
the matrix implementations in Trilinos that run in distributed memory.

Intel Math Kernel Library \cite{mkl} is an efficient and parallel linear
algebra library with matrix operations optimized for Intel
platforms. It provides an efficient sparse matrix multiplication
for regular sparse matrices. In contrast, our sparse matrix multiplication
optimizes for power-law graphs with near-random vertex connection.
