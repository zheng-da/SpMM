Recent sparse matrix multiplication studies focus on in-memory optimizations
for sparse matrix vector multiplication (SpMV).
Williams et al.~\cite{Williams07} describe multiple optimizations for sparse matrix
vector multiplication in multicore architecture. Yoo et al.~\cite{Yoo11}
and Boman et al.~\cite{Boman2013} optimize a distributed SpMV for large
scale-free graphs with 2D edge partitioning to reduce communication between
machines. In contrast, Sparse matrix dense matrix multiplication (SpMM) receives
less attention from the high-performance computing community. Even though
SpMM can be implemented with SpMV, SpMV fails to explore data locality in SpMM.
Aktulga et al.~\cite{Aktulga14} optimize SpMM with cache blocking. We focus on
advancing SpMM with a focus on optimizations for external memory.

Abello et al.~\cite{Abello98} introduced the semi-external memory algorithmic
framework for graphs.  Pearce et al.~\cite{Pearce10} implement several 
semi-external memory graph traversal algorithms for SSDs. FlashGraph
\cite{flashgraph} adopted the concept and performs graph algorithms with
vertex state in memory and edge lists on SSDs. This work extends the semi-external
memory concept to matrix operations. FlashGraph is specifically optimized for
graph algorithms that has a fraction of vertices running in each iteration,
which prevents FlashGraph from performing some optimizations for sparse
matrix multiplication as shown in this paper.

Zhou et al.~\cite{Zhou12} implemented an LOBPCG \cite{Arbenz05} eigensolver in
an SSD cluster. Their implementation targets nuclear many-body Hamiltonian
matrices, which are much denser and have smaller dimensions than many sparse
graphs. Therefore, their solution stores the sparse matrix on SSDs and keep
the entire vector subspace in RAM. Their solution focuses on optimizations
in the distributed environment. In contrast, our eigensolver stores both
the sparse matrix and the vector subspace on SSDs due to the large number
of vertices in our target graphs. We focus on external-memory optimizations
in a single machine.

Anasazi \cite{anasazi} is an eigensolver framework in the Trilinos project
\cite{trilinos}. This framework implements block extension of multiple
eigensolver algorithms
such as Block Krylov-Schur method \cite{krylovschur}, Block Davidson method
\cite{Arbenz05} and LOBPCG \cite{Arbenz05}. This is a very flexible framework
that allows users to redefine sparse matrix dense matrix multiplication and
dense matrix operations. By default, Anasazi uses the matrix implementations
in Trilinos that run in the distributed memory.

Intel Math Kernel Library \cite{mkl} is an efficient and parallel linear
algebra library with matrix operations specifically optimized for the Intel
platform. It also provides an efficient sparse matrix multiplication optimized
for regular sparse matrices. In contrast, our sparse matrix multiplication
routine is specifically optimized for natural graphs with power-law distribution
in vertex degree and near-random vertex connection.
