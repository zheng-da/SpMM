Sparse matrix vector multiplication (SpMV) and sparse matrix dense matrix
multiplication (SpMM) are an important operation in numeric computation and
are well studied in the literature. For example, Williams et. al
\cite{Williams07} described multiple optimizations for sparse matrix
vector multiplication in multicore architecture. Yoo et. al \cite{Yoo11}
and Boman et. al \cite{Boman2013} optimized SpMV for large scale-free graphs
using 2D graph partitioning. Aktulga et. al \cite{Aktulga14} optimized sparse
matrix dense matrix multiplication with cache blocking. In contrast, we
further advance sparse matrix dense matrix multiplication with a focus on
optimizations for external memory.

FlashGraph \cite{flashgraph} is a general graph analysis framework. It performs
graph algorithms in a semi-external memory fashion \cite{Abello98}, i.e., it keeps
vertex state in memory and edge lists on SSDs. It is specifically optimized for
graph algorithms that has a fraction of vertices running in each iteration.
This design prevents FlashGraph from performing some optimizations for sparse
matrix multiplication as shown in this paper.

Anasazi \cite{anasazi} is an eigensolver framework in the Trilinos project
\cite{trilinos}. This framework implements block extension of multiple
eigensolver algorithms
such as Block Krylov-Schur method \cite{krylovschur}, Block Davidson method
\cite{Arbenz05} and LOBPCG \cite{Arbenz05}. This is a very flexible framework
that allows users to redefine sparse matrix dense matrix multiplication and
dense matrix operations. By default, Anasazi uses the matrix implementations
in Trilinos that run in the distributed memory.

Zhou et al. \cite{Zhou12} implemented an LOBPCG \cite{Arbenz05} eigensolver in
an SSD cluster. Their implementation targets nuclear many-body Hamiltonian
matrices, which are much denser and have smaller dimensions than many sparse
graphs. Therefore, their solution stores the sparse matrix on SSDs and keep
the entire vector subspace in RAM. Their solution also focus on optimizations
in the distributed environment. In contrast, we store both the sparse matrix
and the vector subspace on SSDs due to the large number of vertices in
our target graphs. We focus on external-memory optimizations in a single machine.
