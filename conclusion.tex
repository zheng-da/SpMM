We present an alternative solution for scaling sparse matrix dense matrix
multiplication (SpMM) to large sparse matrices by utilizing commodity SSDs
in a large parallel machine. We perform this operation in semi-external memory
(SEM), in which
we keep the sparse matrix on SSDs and the dense matrices in memory. Semi-external
memory increases scalability in proportion to the ratio of non-zero entries
to rows or columns in a sparse matrix. SEM SpMM requires both memory optimizations,
such as cache blocking and NUMA organization, and I/O optimizations, such as I/O
polling and memory buffer pools, to realize performance.

Our SEM SpMM achieves performance comparable to our highly optimized in-memory
implementation while significantly outperforming the Intel MKL and
Trilinos implementations. Our SEM implementation achieves almost 100\% performance
of the in-memory implementation on some graphs when the dense matrices can fit
in memory and have more than four columns. Even when the dense matrix has only
one column, it achieves at least 65\% of the performance of its in-memory counterpart
on different graphs. Our SEM sparse matrix multiplication also scales to very
large graphs with billions of vertices and hundreds of billions of edges.

For a machine with insufficient memory to keep the entire input dense matrix
in memory, we partition the dense matrix vertically and run SEM SpMM multiple
times. In this case, the main overhead of SEM SpMM comes from the loss of
data locality caused by vertical partitioning on the dense matrix. However,
given sufficient memory to keep a small number of columns of the input dense
matrix, we achieve performance comparable to the in-memory counterpart.

We apply our sparse matrix multiplication to three important applications:
PageRank, eigendecomposition and non-negative matrix factorization. We demonstrate
how additional memory should be used in semi-external memory in each application.
We further demonstrate that each of our implementations significantly outperform
state of the art and scale to very large graphs.

Through thorough evaluation, we demonstrate that semi-external memory
coupled with fast SSDs achieves performance very close to highly optimized
in-memory implementations and scales to massive datasets.
As such, our approach provides a very promising alternative to distributed
computation for large-scale data analysis.

Our SSD-based solution also achieves very high energy efficiency even though
we have not measured energy consumption explicitly. SSDs are energy-efficient
storage media \cite{Tsirogiannis} compared with RAM and hard drives.
When processing large datasets, our solution only uses
a single machine and requires a relatively small amount of memory. In contrast,
a distributed solution requires many more machines and much more aggregate
memory in order to process datasets of the same size. As such, our solution
introduces an energy-efficient architecture for large-scale data analysis tasks.
