We present an alternative solution for scaling sparse matrix dense matrix
multiplication (SpMM) to large sparse matrices by utilizing commodity SSDs
in a large parallel machine. We perform this operation in semi-external memory
(SEM), in which we keep the sparse matrix on SSDs and the dense matrices in
memory. Semi-external memory increases scalability in proportion to the ratio
of non-zero entries to rows or columns in a sparse matrix, while achieving
in-memory performance. We demonstrate
that our approach provides a promising alternative to distributed computation
for large-scale data analysis.

With substantial memory and I/O optimizations, our SEM-SpMM achieves high
efficiency while scaling to large graphs with
billions of vertices and hundreds of billions of edges. It significantly
outperforms the Intel MKL and Trilinos implementations. We run our
implementation in memory (IM-SpMM) to quantify the overhead of keeping data
on SSDs. SEM-SpMM achieves almost 100\% performance of IM-SpMM on some graphs
when the dense matrices have more than four columns, and achieves at least 65\%
of the performance of IM-SpMM on all graphs even when the dense matrix
has only one column.

For a machine with insufficient memory to keep the entire input dense matrix
in memory, we partition the dense matrix vertically and run SEM-SpMM multiple
times. In this case, the main overhead of SEM-SpMM comes from the loss of
data locality caused by vertical partitioning on the dense matrix. However,
given sufficient memory to keep a small number of columns of the input dense
matrix, we achieve performance comparable to IM-SpMM.

We apply our sparse matrix multiplication to three important applications:
PageRank, eigendecomposition and non-negative matrix factorization. We demonstrate
how additional memory should be used in semi-external memory in each application.
We further demonstrate that each of our implementations significantly outperforms
state of the art and scales to large graphs.

Our SSD-based solution also achieves high energy efficiency even though
we have not measured energy consumption explicitly. SSDs are energy-efficient
storage media \cite{Tsirogiannis} compared with RAM and hard drives.
When processing large datasets, our solution only uses
a single machine and requires a relatively small amount of memory. In contrast,
a distributed solution requires many more machines and much more aggregate
memory in order to process datasets of the same size. As such, our solution
introduces an energy-efficient architecture for large-scale data analysis tasks.
