We present an alternative solution of scaling sparse matrix multiplication
to large sparse matrices that cannot fit in memory by utilizing commodity SSDs.
We perform sparse matrix multiplication in semi-external memory (SEM), in which
we keep the sparse matrix on SSDs and the dense matrices in memory. Semi-external
memory increases scalability in proportion to the ratio of non-zero entries
to rows or columns in a sparse matrix. It incorporates well with many memory
optimizations for sparse matrix
multiplication such as cache blocking and partitioning the dense matrix for
NUMA machines. We also deploy a set of I/O optimizations for high-speed SSDs
such as avoiding thread context switch and preallocate memory buffers for I/O.

Our SEM sparse matrix multiplication is able to achieve performance
comparable to its in-memory counterparts while significantly outperforming
the MKL and Trilinos implementation. When the dense matrix has only one column,
it achieves at least 60\% performance of its in-memory counterpart on different
graphs. As the number of columns in the dense matrix increases, the performance
gap between the in-memory and SEM implementation shrinks and our SEM
implementation even achieves 100\% performance of the in-memory implementation
on some graphs.
%The semi-external memory
%sparse matrix multiplication is able to saturate either CPU or
%SSDs or both, which suggests that we have achieved the maximal performance from
%the existing hardware.

Semi-external memory requires a machine with the minimum memory to contain
at least one column of the input dense matrix. For a machine with insufficient memory
to keep the entire input dense matrix in memory, we partition the dense matrix
vertically and run semi-external memory sparse matrix multiplication multiple
times. Given a fast SSD array, the SEM sparse matrix multiplication becomes
bottlenecked by CPU, even when the number of columns in the dense matrix
is small. As such, as long as a machine has sufficient memory to contain
a small number of columns of the input dense matrix, we can achieve performance
comparable to the in-memory implementation for a dense matrix with an arbitrary
number of columns. As such, sufficient memory is beneficial for the SEM solution,
but additional memory cannot improve performance further once the system is
bottlenecked by CPU.

We apply our sparse matrix multiplication to three important applications:
PageRank, eigendecomposition and non-negative matrix factorization. We demonstrate
how additional memory should be used in semi-external memory in each application.
In summary, additional memory should always first be used for the input dense
matrix, and then the output dense matrix. If there exists additional memory,
we can further improve performance by caching the sparse matrix.
