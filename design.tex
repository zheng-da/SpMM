\section{SAFS}

SAFS \cite{safs} is a user-space filesystem for a high-speed SSD array in
a NUMA (non-uniform memory architecture) machine. It is implemented as
a library and runs in the address space
of its application. It is deployed on top of the Linux native filesystem.
SAFS was originally designed for optimizing small I/O accesses. However,
sparse matrix multiplication and dense matrix operations
generate much fewer but much larger I/O. Therefore, we provide additional
optimizations to maximize sequential I/O throughput from a large SSD array.

The original SAFS has a dedicated I/O thread for each SSD. The I/O thread
accesses the SSD exclusively to avoid lock contention in the Linux kernel.
Application threads have to send I/O requests to one of the I/O threads
with message passing when accessing data from SSDs. It is necessary to have
one I/O thread for
an SSD when applications issue many small I/O accesses because processing
a large number of I/O accesses consumes a significant number of CPU cycles.
However, when the workload only has large I/O requests, each I/O request takes
much longer time to complete. As a result, the I/O threads are constantly put
into sleep while waiting for I/O and each I/O completion may suffer from
the latency of a thread context switch.

The latency of a thread context switch becomes noticeable on a high-speed SSD
array under a sequential I/O workload and it becomes critical to avoid thread
context switch to gain I/O performance. Therefore, instead of having an I/O
thread for each SSD, we use only a single I/O thread for each NUMA node, which
is responsible for all of the SSDs connected to the NUMA node. As such, an I/O
thread processes many more I/O requests to amortize the latency of a context
switch. Similarly, application threads communicate with I/O threads through
message passing when issuing I/O requests. If the computation in application
threads did not saturate CPU, SAFS would put the application threads into
sleep while they were waiting for I/O. This results in many thread context
switches and underutilization of both CPU and SSDs. To saturate I/O,
an application thread issues asynchronous I/O and poll for I/O to avoid thread
context switches after completing all computation available to it.

To better support access to many relatively small files simultaneously, SAFS
stripes data in a file across SSDs with a different striping order for each file.
This strategy stores data from multiple files evenly across SSDs and improves
I/O utialization. Due to the sequential I/O workload, FlashEigen stripes data
across SSDs with a large block size, on the order of megabytes, to increase I/O
throughput and potentially reduce write amplification on SSDs \cite{Tang15}.
Such a large block size may cause storage skew for small files
on a large SSD array if every file stripes data in the same order. Using
the same striping order for all files may also cause skew in I/O access.
Therefore, SAFS generates a random striping order for each file to evenly
distribute I/O among SSDs when a file is created. SAFS stores the striping
order with the file for future data retrieval.

\section{Sparse matrix multiplication}
Sparse matrix multiplication on graphs usually leads to many random memory
accesses and its performance is usually limited by the random memory performance
of DRAM. To scale sparse matrix multiplication to a sparse graph with billions
of vertices, we perform this operation in semi-external memory (SEM). That is,
we keep dense matrices in memory and the sparse
matrix on SSDs. This strategy enables nearly in-memory performance while achieving
the scalability in proportion to the ratio of edges to vertices in a graph.

\subsection{The sparse matrix format}
The state-of-art numeric libraries store a sparse matrix in compressed row storage
(CSR) or compressed column storage (CSC) format. However, these formats incur
many CPU cache misses in sparse matrix multiplication on many real-world graphs
due to their nearly random vertex connection. They also require a relatively
large storage size. For a graph with billions of edges, we have to use eight
bytes to store the row and column indices. For semi-external memory sparse
matrix multiplication, SSDs may become the bottleneck if a sparse matrix has
a large storage size.
Therefore, we need to use an alternative format for sparse matrices to increase
CPU cache hits and reduce the amount of data read from SSDs.

\begin{figure}
\centering
\includegraphics[scale=0.3]{./sparse_mat.pdf}
\caption{The format of a sparse matrix.}
\label{sparse_mat}
\end{figure}

To increase CPU cache hits, we deploy cache blocking \cite{Im04} and store
non-zero entries of a sparse matrix in tiles (Figure \ref{sparse_mat}).
When a tile is small, the rows from the input and output dense matrices
involved in the multiplication with the tile are always kept in the CPU cache
during the multiplication. The optimal tile size should fill the CPU cache
with the rows of the dense matrices involved in the multiplication with
the tile and is affected by the number of columns of the dense matrices,
which is chosen by users. Instead of generating a sparse matrix with
different tile sizes optimized for different numbers of columns in the dense
matrices, we use a relatively small tile size and rely on the runtime system
to optimize for different numbers of columns (in section \ref{sec:exec}).
In the semi-external memory, we expect that the dense matrices do not
have more than eight columns in sparse matrix multiplication. Therefore, we
use the tile size of $16K \times 16K$ by default to balance the matrix storage
size and the adaptibility to different numbers of columns.

\begin{figure}
\centering
\includegraphics[scale=0.5]{./tile_format.pdf}
\caption{The storage format of a tile in a sparse matrix.}
\label{tile_format}
\end{figure}

To reduce the overall storage size of a sparse matrix, we use a compact format
to store non-zero entries in a tile. In very sparse matrices such as
many real-world graphs, many rows in a tile do not have any non-zero entries.
The CSR (CSC) format requires an entry for each row (column) in the row
(column) index. Therefore, the CSR or CSC format wastes space when storing elements
in a tile. Instead, we only keep data for rows with non-zero entries in a tile
shown in Figure \ref{tile_format} and refer to this format as SCSR (Super
Compressed Row Storage). This format maintains a row header for each non-empty
row. A row header has an identifier to indicate the row number, followed by
column indices. 
The most significant bit of the identifier is always set to 1, while the most
significant bit of a column index entry is always set to 0. As such, we can easily
distinguish a row identifier from a column index entry and determine the end
of a row. Thanks to the small size of a tile, we use two bytes to further store a row
number and a column index entry to reduce the storage size. Since the most
significant bit is used to indicate the beginning of a row, this format allows
a maximum tile size of $32K \times 32K$.

For many real-world graphs, many rows in a tile have only one non-zero entry,
thanks to the sparsity of the graphs and nearly random vertex connection.
Iterating over single-entry rows requires to test the end of a row for every
non-zero entry, resulting in many extra conditional jump CPU instructions
in sparse matrix multiplication.
In contrast, the coordinate format (COO) is more suitable for storing these
single-entry rows. It does not increase the storage size but significantly
reduces the number of conditional jump instructions when we iterate
them. As a result, we hybrid SCSR and COO to store non-zero entries in a tile
with COO stored behind the row headers of SCSR. All non-zero entries are
stored together at the end of a tile.

We organize tiles in a sparse matrix in tile rows and maintain a matrix index
for them. Each entry of the index stores the location of a tile row on SSDs
to facilitate random access
to tile rows. This is useful for parallelizing sparse matrix multiplication.
Because a tile contains thousands of rows, the matrix index requires a very
small storage size even for a billion-node graph. We keep the entire index
in memory during sparse matrix multiplication.

\subsection{The dense matrices in sparse matrix multiplication}
Dense matrices in sparse matrix multiplication are tall-and-skinny matrices
with millions or even billions of rows but only a small number of columns.
The number of rows in a dense matrix is determined by the number of vertices
in a sparse graph and the number of columns is determined by applications.
The dense matrix is kept in memory for semi-external memory (SEM) sparse matrix
dense matrix multiplication (SpMM),
so the size of the input dense matrix governs memory consumption
of SpMM. Given the limited amount of RAM in a machine, the number of columns
in a dense matrix has to be small.

For a non-uniform memory architecture (NUMA), we partition the input dense matrix
horizontally and store partitions evenly across NUMA nodes to fully utilize
the bandwidth of memory and inter-processor links in sparse matrix
multiplication. The NUMA architecture is prevalent in today's multi-processor
servers, where each processor connects to its own memory banks. As shown in
Figure \ref{dense_mat} (a), we assign multiple
contiguous rows in a row interval to a partition, which is assigned to a NUMA
node. A row interval always has $2^i$ rows for efficiently locating a row
with bit operations. The row interval size is also multiple of the tile size of
a sparse matrix so that multiplication on a tile only needs to access rows
from a single row interval. The elements in the input dense matrix are stored
in row-major order to increase data locality in SpMM.

\begin{figure}
\centering
\includegraphics[scale=0.4]{./dense_matrix.pdf}
\caption{The data layout of tall-and-skinny (TAS) dense matrices. A TAS
dense matrix is partitioned horizontally into many row intervals.
(a) For an in-memory matrix, row intervals are stored across NUMA nodes and
elements are stored in row-major order; (b) for an SSD-based matrix, elements
inside a row interval are stored in column-major order.}
\label{dense_mat}
\end{figure}

\subsection{Execution of sparse matrix multiplication} \label{sec:exec}
We perform sparse matrix dense matrix multiplication in semi-external memory
and optimize it for different numbers of columns in the dense matrices.
Thanks to semi-external memory, sparse matrix multiplication streams data
in the sparse matrix from SSDs, which maximizes I/O throughput of the SSDs.

To better utilize CPU cache, we process tiles of a partition in
\textit{super tile}s (Figure \ref{sparse_mat}). The tile size of a sparse
matrix is specified when the sparse matrix image is created and is relatively
small to handle different numbers of columns in the dense matrices.
A \textit{super tile} is composed of tiles from multiple tile rows and its
size is determined at runtime by three factors: the number of columns
in the dense matrices, the CPU cache size and the number of threads that
share the CPU cache. An optimal size for a \textit{super tile} fills
the CPU cache with the rows from the dense matrices involved in
the computation with the \textit{super tile}.

We partition a sparse matrix horizontally for parallelization (Figure
\ref{sparse_mat}). When a thread gets tile rows, it reads them asynchronously
from SSDs and processes them completely independently. Once a partition
is ready in memory, the worker thread multiplies the partition with the input
dense matrix. A thread processes one \textit{super tile} at a time. It stores
the intermediate result in a buffer allocated in the local memory to reduce
remote memory access when it goes through all the tiles in the partition.

We maintain a global task queue for sparse matrix multiplication and a worker
thread gets one task at a time from the queue. A task may
indicates the computation on a \textit{super tile} row or a single tile row.
At the beginning of the computation, threads get larger tasks; when
the computation get close to the end, threads get smaller tasks. This strategy
achieves good load balancing. The other benefit of maintaining a global
task queue is to maintain the global execution order, which becomes essential
when the output dense matrix needs to be written to SSDs. When a thread gets
a task, the final computation result from the task may be small. Instead of
writing the computation result immediately whenever it is generated, we merge
computation results from multiple threads and write them with a single I/O.
As such, we need to maintain a global execution order to help us merge.

There are three options of keeping the output dense matrix of SpMM. In some
applications, we can keep the output dense matrix in memory if a machine has
sufficient memory. Otherwise, we write the output dense matrix to SSDs.
In this case, we stream the output dense matrix to SSDs when data in a portion
is generated. Horizontal partitioning ensures that the data written to SSDs is
always the final result of sparse matrix multiplication. Another option is to
stream the data to the subsequent operations when data is ready. \dz{I need to
implement this.} As such, we minimize the data written to SSDs.

In spite of nearly random edge connection in a real-world graph,
there exists regularity that allows vectorization to improve performance
in sparse matrix dense matrix multiplication. For each non-zero entry, we
need to multiply it with the corresponding row from the input dense matrix
and add the result to the corresponding row in the output dense matrix.
These operations can be accomplished by the vector CPU instructions such as
AVX \cite{avx}. The current implementation relies on GCC's auto-vectorization
to translate the C code to the vector CPU instructions by predefining the matrix
width in the code.

When accessing a sparse matrix on SSDs, we keep a set of memory buffers for
I/O access to reduce the overhead of memory allocation.
For a large spare matrix, each tile row is fairly large, on the order
of tens of megabytes. The operating system usually allocate a memory buffer
for such an I/O size with \textit{mmap()} and populates the buffer with physical
pages when the buffer is used. It is computationally expensive to populate
large memory buffers frequently. When accessing high-throughput I/O devices,
such overhead can cause substantial performance loss. Therefore, we keep a set
of memory buffers allocated previously and reuse them for new I/O requests.
Because tile rows in a sparse matrix usually have differnt sizes, we resize
a previously allocated memory buffer if it is too small for a new I/O request.

In some applications such as non-negative matrix factorization, the dense matrices
involved in sparse matrix multiplication may not fit in memory. We partition
the dense matrix vertically so that each partition has complete columns and can
fit in memory. We also organize each partition in the row-major order to increase
data locality. As such, we perform sparse matrix multiplication multiple times
to compute the final result. This approach requires $\lceil \frac{D}{M} \rceil$
passes over the sparse matrix. When RAM becomes the bottleneck, it does not
really matter.

\subsection{Caching in sparse matrix multiplication}
In the hardware where memory capacity exceeds the storage size of a vector, we
can use additional memory to cache a portion of the sparse matrix if sparse matrix
multiplication is required in an iterative algorithm. We cannot rely on
the page cache in SAFS \cite{sa-cache} to buffer some portion of the sparse matrix
because streaming a sparse matrix to memory always evict existing data in the page
cache and generates zero cache hits. Therefore, we explicitly cache some portion
of the sparse matrix in sparse matrix multiplication.

\subsection{The impact of the memory size on I/O in semi-external memory}
The memory size has significantly impact on I/O in semi-external memory.
The minimum memory requirement for semi-external memory sparse matrix
multiplication is $n \times c + \epsilon$, where $n$ is the number of rows or
columns of the sparse matrix, $c$ is the size of an element in the dense matrix,
and $\epsilon$ is the buffer size for part of the sparse matrix and the output
dense matrix.

When the input dense matrix cannot fit in memory, we should use the existing memory
to keep as many columns in the input dense matrix in memory as possible. Although
we can cache part of the sparse matrix, keeping more columns in memory saves more
I/O than using the same RAM to cache the sparse matrix. If all memory is used to
store the input dense matrixThe amount of data read from SSDs is
$\lceil \frac{n \times c \times k}{M} \rceil \times S$, where $k$ is the number
of columns in the dense matrix, $M$ is the size of memory, $S$ is the storage
size of the sparse matrix. In contrast, if all memory is used to cache the sparse
matrix, the amount of data read from SSDs is $[S - (M - n \times c)] \times k$.
$[S - (M - n \times c)] \times k > \lceil \frac{n \times c \times k}{M} \rceil \times S$,
if $S > M$ and $M > n \times c$.

Our strategy results in the minimum amount of data written to SSDs. When the output
dense matrix cannot fit in memory, the maximum write is $n \times c \times k$.
In other words, the output matrix only needs to be written to SSDs at most once.
The additional memory in the system can be used to buffer part of the output
dense matrix to reduce the amount of data written to SSDs.

As the number of columns kept in memory increases, the bottleneck of the system
may switch. When we can keep only one column of the input dense matrix in memory,
the system may be I/O bound; when we can keep more columns of the dense matrix
in memory, the system will become CPU or memory-bound. When the system becomes
CPU or memory-bound, the I/O complexity becomes irrelevant. The additional memory
should be used for buffering the result of sparse matrix multiplication.

\section{Applications}
We apply sparse matrix multiplication to three important applications widely
used in data mining and machine learning: PageRank \cite{pagerank},
eigendecomposition \cite{anasazi} and non-negative matrix factorization \cite{nmf}.
Each application requires a slightly different strategy of using memory in
sparse matrix multiplication.

\subsection{PageRank}
PageRank is an algorithm to rank the Web pages by using hyperlinks between Web
pages. It was first used by Google and is identified as one of the top 10 data
mining algorithms \cite{top10}. The algorithm runs iteratively and its update
rule for each Web pages in each iteration is
$PR(u) = \frac{1-d}{N} + d(\sum\limits_{v \in B(u)} \frac{PR(v)}{L(v)})$,
where $B(u)$ denotes the neighbor list of vertex $u$ and $L(v)$ denotes
the out-degree of vertex $v$. The PageRank algorithm can be expressed as sparse
matrix multiplication with the code below.

\vspace{-5pt}
\begin{minted}[mathescape, fontsize=\scriptsize,]{r}
pr2 <- (1-d)/N+d*A%*%(pr1/deg)
\end{minted}
\vspace{-5pt}

Based on the memory size, we can place different data in memory to reduce I/O.
When the memory can only fit a single vector, each iteration needs
to write a vector to SSDs and read two vectors (the result from
the previous iteration and the degree vector) and the sparse matrix
from SSDs.
When the memory can fit two vectors, the output vector can be kept
in memory, so each iteration needs to read the sparse matrix and only
one vector.
As more memory can be used, we can further keep the degree vector
and even cache part of the sparse matrix.

\subsection{Eigendecomposition}
Eigendecomposition and singular value decomposition (SVD) is commonly used
in many scientific fields as well as machine learning and data mining. Many
algorithms \cite{Lanczos, IRLM, krylovschur} and frameworks
\cite{arpack, anasazi, slepc} have been developed to solve a large eigenvalue
problem.

We take advantage of the Anasazi eigensolver framework \cite{anasazi} and
replace its original matrix operations with our semi-external memory sparse
matrix multiplication and external-memory dense matrix operations. To compute
eigenvalues of a $n \times n$ matrix, many eigenvalue algorithms for a large
sparse matrix require to construct a vector subspace with a sequence of
sparse matrix multiplication. Each vector in the subspace has the length of $n$.
For many sparse graphs such as social networks, the vector subspace requires
substantial storage size. Therefore, we need to keep part of the subspace
on SSDs. Eigensolvers perform some dense matrix operations on the subspace.
For example, eigensolvers need to orthogonalize the vectors in the subspace,
which requires dense matrix multiplication. The Anasazi eigensolvers have
block extension to update multiple
vectors in the subspace simultaneously and thus require sparse matrix dense
matrix multiplication. The most efficient Anasazi eigensolver on sparse graphs
is the KrylovSchur eigensolver \cite{krylovschur}, which updates a small number
of vectors (1-4) in the subspace simultaneously. Zheng et al.
\cite{flasheigen} provides the details of extending the Anasazi eigensolver
with external-memory operations.

The choice of data placement for an eigensolver is a little different from
PageRank. If a machine has more memory, the memory should be used to keep
		the input and output dense matrices. The dense matrices involved in
		sparse matrix multiplication have a small number of columns in
		the KrylovSchur eigensolver and usually can fit in memory. 
Additional memory should be used to buffer the vectors in the subspace
		to reduce I/O for dense matrix operations.

\subsection{Non-negative matrix factorization}
Non-negative matrix factorization (NMF) \cite{nmf} is to find two non-negative
low-rank matrices $W$ and $H$ to approximate a matrix $A \approx WH$. NMF is
typically used to factorize sparse matrices. NMF has many applications in
machine learning
and data mining. A well-known example is collaborative filtering \cite{cf} in
recommender systems. NMF is also applied to graphs, for example, for community
detection \cite{yang13, wang11}.

Many algorithms were designed to solve NMF and here we describe an algorithm
\cite{nmf} that requires a sequence of sparse matrix multiplication.
The algorithm use multiplicative update rules and update matrices $W$ and $H$
alternately. When updating $H$, the algorithm fixes $W$ and updates $H$.
Similarly, the algorithm fixes $H$ to update $W$.

$H_{a\mu} \leftarrow H_{a\mu} \frac{{(W^TA)}_{a\mu}}{{(W^TWH)}_{a\mu}}$,
$W_{ia} \leftarrow W_{ia} \frac{{(AH^T)}_{ia}}{{(WHH^T)}_{ia}}$

When applying NMF to a very sparse graph, the non-negative matrices $W$ and $H$
may not completely fit in memory. As such, we apply semi-external memory sparse
matrix multiplication to the algorithm above differently based on the memory size
and the number of columns in $W$ and $H$. If the memory is not large enough to
keep $W$ and $H$, we need to partition $W$ and $H$ vertically and run multiple
sparse matrix multiplications to compute $W^TA$ and $AH^T$. The choices of data
placement for NMF are shown as follows.
When memory is small, all memory should be used to keep as many
columns in the input dense matrix as possible to reduce I/O.
When the memory size is sufficiently large to cause memory to be
the bottleneck, additional memory should be used to buffer the output
matrix.
